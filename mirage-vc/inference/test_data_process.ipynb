{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Obtain reference data for three viewing angles of the test sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （1） Other companies with similar basic information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pickle, json, numpy as np, pandas as pd, torch\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "csv_path  = Path('../test_data_sampled.csv')\n",
    "test_pkl  = Path('../test_data_2022_basic.pkl')\n",
    "train_pkl  = Path('../train_data_2022_basic.pkl')\n",
    "out_path  = Path('../test_to_similar.json')\n",
    "\n",
    "df = pd.read_csv(csv_path, dtype={'CompanyID': str})\n",
    "df = df[(df['time'] >= 177) & (df['time'] <= 192)]\n",
    "comp2time = dict(zip(df['CompanyID'], df['time']))\n",
    "\n",
    "query_ids = list(comp2time.keys())\n",
    "print(f\"Total query companies (time 50-190): {len(query_ids)}\")\n",
    "\n",
    "with test_pkl.open('rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "with train_pkl.open('rb') as f:\n",
    "    hist_data = pickle.load(f)\n",
    "\n",
    "all_data = {}\n",
    "all_data.update(hist_data)\n",
    "all_data.update(test_data)         \n",
    "\n",
    "candidate_keys, candidate_texts, candidate_times = [], [], []\n",
    "missing_basic = 0\n",
    "for cid, t in comp2time.items():\n",
    "    info = all_data.get(cid)\n",
    "    if info and 'basic_info' in info and info['basic_info']:\n",
    "        candidate_keys.append(cid)\n",
    "        candidate_texts.append(info['basic_info'])\n",
    "        candidate_times.append(t)\n",
    "    else:\n",
    "        missing_basic += 1\n",
    "\n",
    "print(f\"Candidates w/ basic_info : {len(candidate_keys)}\")\n",
    "if missing_basic:\n",
    "    print(f\"‼  {missing_basic} companies skipped (basic_info missing)\")\n",
    "\n",
    "candidate_times = np.array(candidate_times, dtype=np.int16)\n",
    "\n",
    "local_model_dir = \"../models--sentence-transformers--all-MiniLM-L6-v2\"\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = SentenceTransformer(\n",
    "    local_model_dir,\n",
    "    device=device,\n",
    "    local_files_only=True  \n",
    ")\n",
    "\n",
    "batch_size = 512\n",
    "cand_embs  = []\n",
    "for i in tqdm(range(0, len(candidate_texts), batch_size), desc='Encoding candidates'):\n",
    "    batch = candidate_texts[i:i+batch_size]\n",
    "    emb   = model.encode(batch,\n",
    "                         batch_size=len(batch),\n",
    "                         convert_to_numpy=True,\n",
    "                         normalize_embeddings=True,\n",
    "                         show_progress_bar=False)\n",
    "    cand_embs.append(emb)\n",
    "cand_embs = np.vstack(cand_embs).astype('float32')  \n",
    "\n",
    "print(f\"Candidate embeddings shape: {cand_embs.shape}\")\n",
    "\n",
    "result = {}\n",
    "for qid in tqdm(query_ids, desc='Searching'):\n",
    "    q_info = all_data.get(qid)\n",
    "    if (not q_info) or ('basic_info' not in q_info):\n",
    "        continue\n",
    "\n",
    "    q_time = comp2time[qid]\n",
    "    q_emb  = model.encode(q_info['basic_info'],\n",
    "                          convert_to_numpy=True,\n",
    "                          normalize_embeddings=True)\n",
    "\n",
    "    mask = candidate_times <= q_time\n",
    "\n",
    "    if qid in candidate_keys:\n",
    "        self_idx = candidate_keys.index(qid)\n",
    "        mask[self_idx] = False\n",
    "\n",
    "    if not mask.any():\n",
    "        continue\n",
    "\n",
    "    sims = cand_embs[mask] @ q_emb          \n",
    "    masked_keys = np.array(candidate_keys)[mask]\n",
    "\n",
    "    top_k = 4 if sims.shape[0] >= 4 else sims.shape[0]\n",
    "    top_idx = np.argpartition(-sims, top_k-1)[:top_k]\n",
    "    top_idx = top_idx[np.argsort(-sims[top_idx])]     \n",
    "\n",
    "    result[qid] = masked_keys[top_idx].tolist()\n",
    "\n",
    "with out_path.open('w', encoding='utf-8') as f:\n",
    "    json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nDone! Mapping saved → {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （2） Basic information of the lead investor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query companies: 2510\n",
      "Finished. Got investors for 2510 companies.\n",
      "Saved → /data/VC_LLM_Agent/multi_agent/test_data_sampled/4_agent/test_company_to_investor.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pickle, json, networkx as nx, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "strat_csv     = Path('../test_data_sampled.csv')\n",
    "time_csv      = Path('../company_time_id.csv')\n",
    "graph_pkl     = Path('../graph_2022_invest.pkl')\n",
    "out_json_path = Path('../test_company_to_investor.json')\n",
    "\n",
    "df_q = pd.read_csv(strat_csv, dtype={'CompanyID': str})\n",
    "df_q = df_q[(df_q['time'] >= 177) & (df_q['time'] <= 192)]\n",
    "query_ids      = set(df_q['CompanyID'])\n",
    "query_time_map = dict(zip(df_q['CompanyID'], df_q['time']))\n",
    "print(f\"Query companies: {len(query_ids)}\")\n",
    "\n",
    "df_time = pd.read_csv(time_csv, dtype={'CompanyID': str}).drop_duplicates(subset=['CompanyID'])\n",
    "time_map = dict(zip(df_time['CompanyID'], df_time['time']))\n",
    "time_map.update(query_time_map)          \n",
    "\n",
    "with graph_pkl.open('rb') as f:\n",
    "    G: nx.MultiGraph = pickle.load(f)\n",
    "\n",
    "# real-id → node-idx\n",
    "id_to_node = {attr['id']: n for n, attr in G.nodes(data=True)}\n",
    "\n",
    "def is_person_id(rid: str) -> bool:\n",
    "    return rid.endswith('P')\n",
    "\n",
    "result = {}   # company_id → {'invest_person': ..., 'time': ...}\n",
    "\n",
    "for cid in query_ids:\n",
    "    c_time = time_map.get(cid)\n",
    "    node   = id_to_node.get(cid)\n",
    "    if (c_time is None) or (node is None):\n",
    "        result[cid] = {'invest_person': None, 'time': None}\n",
    "        continue\n",
    "\n",
    "    best_investor, best_date = None, -1\n",
    "\n",
    "    for u, v, k, attr in G.edges(node, keys=True, data=True):\n",
    "        edge_date = attr.get('edge_date')\n",
    "        if edge_date is None:\n",
    "            continue\n",
    "        try:\n",
    "            edge_date = int(edge_date)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        if edge_date > c_time:\n",
    "            continue                         \n",
    "\n",
    "        other = v if u == node else u\n",
    "        other_id = G.nodes[other]['id']\n",
    "        if not is_person_id(other_id):\n",
    "            continue                         \n",
    "\n",
    "        if edge_date > best_date:\n",
    "            best_date     = edge_date\n",
    "            best_investor = other_id\n",
    "\n",
    "    result[cid] = {'invest_person': best_investor, 'time': int(c_time)}\n",
    "\n",
    "print(f\"Finished. Got investors for {len(result)} companies.\")\n",
    "\n",
    "with out_json_path.open('w', encoding='utf-8') as f:\n",
    "    json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved → {out_json_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （3） Graph reasoning path analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "input_csv = Path('../test_data_sampled.csv')         \n",
    "out_json  = Path('../test_data_sampled_path.json')    \n",
    "\n",
    "df_in = pd.read_csv(input_csv, dtype={'CompanyID': str})\n",
    "\n",
    "if 'time' in df_in.columns:\n",
    "    df_q = df_in[(df_in['time'] >= 50) & (df_in['time'] <= 190)].copy()\n",
    "else:\n",
    "    df_q = df_in.copy()\n",
    "\n",
    "query_ids = set(df_q['CompanyID'])\n",
    "print(f\"Target companies (time in [50,190] if available): {len(query_ids)}\")\n",
    "\n",
    "path1_series = df_in.get('Path1', pd.Series([None] * len(df_in)))\n",
    "path2_series = df_in.get('Path2', pd.Series([None] * len(df_in)))\n",
    "path_map = dict(zip(df_in['CompanyID'], zip(path1_series, path2_series)))\n",
    "\n",
    "PATH_SEP = '|'\n",
    "\n",
    "def _is_nan(x) -> bool:\n",
    "    return x is None or (isinstance(x, float) and math.isnan(x))\n",
    "\n",
    "def parse_path(path_str):\n",
    "    if _is_nan(path_str):\n",
    "        return None\n",
    "    s = str(path_str).strip()\n",
    "    if not s:\n",
    "        return None\n",
    "    return [tok.strip() for tok in s.split(PATH_SEP) if tok.strip()]\n",
    "\n",
    "def check_alternating(path_ids):\n",
    "    if not path_ids or len(path_ids) < 2:\n",
    "        return True\n",
    "    def is_person_id(rid: str) -> bool:\n",
    "        return rid.endswith('P')\n",
    "    last_is_person = is_person_id(path_ids[0])\n",
    "    for rid in path_ids[1:]:\n",
    "        cur_is_person = is_person_id(rid)\n",
    "        if cur_is_person == last_is_person:\n",
    "            return False\n",
    "        last_is_person = cur_is_person\n",
    "    return True\n",
    "\n",
    "result = {} \n",
    "missing_in_paths = 0\n",
    "bad_alt_count = 0\n",
    "both_ok = 0\n",
    "\n",
    "for cid in query_ids:\n",
    "    p1_str, p2_str = path_map.get(cid, (None, None))\n",
    "    p1 = parse_path(p1_str)\n",
    "    p2 = parse_path(p2_str)\n",
    "\n",
    "    if (p1 is None) and (p2 is None):\n",
    "        missing_in_paths += 1\n",
    "\n",
    "    if p1 and not check_alternating(p1):\n",
    "        bad_alt_count += 1\n",
    "    if p2 and not check_alternating(p2):\n",
    "        bad_alt_count += 1\n",
    "    if p1 and p2:\n",
    "        both_ok += 1\n",
    "\n",
    "    result[cid] = {'path1': p1, 'path2': p2}\n",
    "\n",
    "print(f\"Finished. Companies exported: {len(result)}\")\n",
    "print(f\" - Not found (both paths missing): {missing_in_paths}\")\n",
    "print(f\" - Alternation rule violations (non-fatal): {bad_alt_count}\")\n",
    "print(f\" - Both paths present: {both_ok}\")\n",
    "\n",
    "out_json.parent.mkdir(parents=True, exist_ok=True)\n",
    "with out_json.open('w', encoding='utf-8') as f:\n",
    "    json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "print(f\"Wrote JSON to: {out_json.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Constructing three angle prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （1） Here are some tips for building information based on companies with similar backgrounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "mapping_json = Path('../test_to_similar.json')\n",
    "test_pkl     = Path('../test_data_2022_basic.pkl')\n",
    "hist_pkl     = Path('../train_data_2022_basic.pkl')\n",
    "out_pkl      = Path('../text_similar_company_prompt.pkl')\n",
    "\n",
    "with mapping_json.open('r', encoding='utf-8') as f:\n",
    "    mapping = json.load(f)\n",
    "test_ids = list(mapping.keys())\n",
    "\n",
    "with test_pkl.open('rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "with hist_pkl.open('rb') as f:\n",
    "    hist_data = pickle.load(f)\n",
    "\n",
    "all_data = {}\n",
    "all_data.update(hist_data)\n",
    "all_data.update(test_data)\n",
    "\n",
    "intro = (\n",
    "    \"You are a seasoned venture-capital investor. \"\n",
    "    \"A target company has just secured its Series-A financing. \"\n",
    "    \"Your task is to predict whether it will obtain a second round of financing, \"\n",
    "    \"IPO, or be acquired within the next 12 months. \"\n",
    "    \"Below are reference companies (Q) and their outcomes (A). \"\n",
    "    \"In the labels, True means the company succeeded within one year; False means it did not.\\n\"\n",
    ")\n",
    "\n",
    "closing = (\n",
    "    \"### Instructions\"\n",
    "    \"Based on the reference Q-A pairs above and the target company information,\"\n",
    "    \"1. First output your **single-word judgment** on whether the company will obtain a second round, IPO, or be acquired within 12 months.\"\n",
    "    \"   Use exactly one of the following formats:\"\n",
    "    \"      Prediction: True\"\n",
    "    \"      Prediction: False\"\n",
    "    \"\"\n",
    "    \"2. Immediately after that, provide your explanation covering both **positive factors** and **negative factors** that led you to this judgment.\"\n",
    "    \"   You may write as many sentences as you feel necessary.\"\n",
    "    \"\"\n",
    "    \"### Output Format (exactly)\"\n",
    "    \"Prediction: True/False\"\n",
    "    \"Analysis:\"\n",
    "    \"<your detailed analysis here>\"\n",
    ")\n",
    "\n",
    "def bool_str(label: int) -> str:\n",
    "    return \"True\" if label == 1 else \"False\"\n",
    "\n",
    "prompts = {}   \n",
    "\n",
    "for cid in test_ids:\n",
    "    ref_ids = mapping.get(cid, [])[:4]\n",
    "    qa_blocks = []\n",
    "\n",
    "    for rid in ref_ids:\n",
    "        ref_obj = all_data.get(rid)\n",
    "        if not ref_obj:\n",
    "            continue\n",
    "        q = ref_obj.get('basic_info', '').strip()\n",
    "        a = bool_str(ref_obj.get('label', 0))\n",
    "        qa_blocks.append(f\"Q: {q}\\nA: {a}\\n\")\n",
    "\n",
    "    target_obj = all_data.get(cid, {})\n",
    "    target_q   = target_obj.get('basic_info', '').strip()\n",
    "    target_section = (\n",
    "        \"Below is the target company's basic information:\\n\"\n",
    "        f\"Q: {target_q}\\n\"\n",
    "    )\n",
    "\n",
    "    prompt = (\n",
    "        f\"{intro}\\n\"\n",
    "        + \"\\n\".join(qa_blocks)\n",
    "        + \"\\n\"\n",
    "        + target_section\n",
    "        + \"\\n\"\n",
    "        + closing\n",
    "    )\n",
    "\n",
    "    prompts[cid] = {\"input_prompt\": prompt}\n",
    "    \n",
    "with out_pkl.open('wb') as f:\n",
    "    pickle.dump(prompts, f)\n",
    "\n",
    "print(f\"Generated prompts for {len(prompts)} companies → {out_pkl}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （2） Here we build a prompt based on the background analysis of the lead investor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from pathlib import Path\n",
    "import json, pickle, random, sys\n",
    "from collections import defaultdict\n",
    "\n",
    "company_json = Path('../test_company_to_investor.json')\n",
    "train_pkl    = Path('../train_data_2022_basic.pkl')\n",
    "test_pkl     = Path('../test_data_2022_basic.pkl')\n",
    "graph_pkl    = Path('../graph_2022.pkl')\n",
    "out_pkl      = Path('../company_prompts_single_investor.pkl')\n",
    "\n",
    "company_map = json.loads(company_json.read_text())\n",
    "\n",
    "with train_pkl.open(\"rb\") as f:\n",
    "    train_data = pickle.load(f)\n",
    "with test_pkl.open(\"rb\") as f:\n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "basic_info_db = {**train_data, **test_data}\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "with graph_pkl.open(\"rb\") as f:\n",
    "    G: nx.MultiDiGraph = pickle.load(f) \n",
    "\n",
    "id2index = {str(d[\"id\"]): n for n, d in G.nodes(data=True)}\n",
    "\n",
    "def label_str(lbl: int) -> str:\n",
    "    return \"True\" if lbl == 1 else (\"False\" if lbl == 0 else \"Unknown\")\n",
    "\n",
    "\n",
    "def fetch_basic(cid: str):\n",
    "    rec = basic_info_db.get(cid, {})\n",
    "    return rec.get(\"basic_info\", \"\").strip(), label_str(rec.get(\"label\"))\n",
    "\n",
    "\n",
    "def collect_person_history(\n",
    "    person_nd: int, cutoff_time: int, target_cid: str, top_k: int = 5\n",
    "):\n",
    "    invest, position = [], []\n",
    "\n",
    "    for _, nbr, data in G.edges(person_nd, data=True):\n",
    "        edge_time = data.get(\"edge_date\")\n",
    "        if edge_time is None or edge_time >= cutoff_time:\n",
    "            continue  \n",
    "\n",
    "        cid_nbr = str(G.nodes[nbr][\"id\"])\n",
    "        if cid_nbr == target_cid:\n",
    "            continue  \n",
    "\n",
    "        edge_type = str(data.get(\"edge_type\"))  \n",
    "        if edge_type == \"0\":\n",
    "            invest.append(nbr)\n",
    "        else:\n",
    "            position.append(nbr)\n",
    "\n",
    "    random.shuffle(invest)\n",
    "    random.shuffle(position)\n",
    "    return invest[:top_k], position[:top_k]\n",
    "\n",
    "\n",
    "def build_prompt(person_id: str, target_cid: str, target_time: int, target_info: str):\n",
    "    p_nd = id2index.get(person_id)\n",
    "    if p_nd is None:\n",
    "        raise RuntimeError(\"Investor node not found in graph\")\n",
    "\n",
    "    invest_nodes, pos_nodes = collect_person_history(\n",
    "        p_nd, cutoff_time=target_time, target_cid=target_cid\n",
    "    )\n",
    "\n",
    "    invest_lines = []\n",
    "    for i, n in enumerate(invest_nodes, 1):\n",
    "        cid = str(G.nodes[n][\"id\"])\n",
    "        info, lbl = fetch_basic(cid)\n",
    "        if info:\n",
    "            invest_lines.append(f\"  • Deal {i}: {info} (Label: {lbl})\")\n",
    "\n",
    "    position_lines = []\n",
    "    for i, n in enumerate(pos_nodes, 1):\n",
    "        cid = str(G.nodes[n][\"id\"])\n",
    "        info, lbl = fetch_basic(cid)\n",
    "        if info:\n",
    "            position_lines.append(f\"  • Role {i}: {info} (Label: {lbl})\")\n",
    "\n",
    "    if not invest_lines and not position_lines:\n",
    "        raise RuntimeError(\"All history filtered – empty prompt\")\n",
    "\n",
    "    return \"\\n\".join(\n",
    "        [\n",
    "            \"You are an independent venture-capital analyst.\",\n",
    "            f\"Below is the historical track record of **Investor {person_id}**, \"\n",
    "            f\"who is the **lead investor** in the target company's just-closed Series-A round.\",\n",
    "            f\"Only deals *before month {target_time}* are shown to avoid forward-looking bias.\",\n",
    "            \"\",\n",
    "            \"=== Investment History (chronological) ===\",\n",
    "            *(invest_lines or [\"  (none)\"]),\n",
    "            \"\",\n",
    "            \"=== Board / Executive Positions ===\",\n",
    "            *(position_lines or [\"  (none)\"]),\n",
    "            \"\",\n",
    "            \"=\" * 60,\n",
    "            \"\",\n",
    "            \"### Target Company\",\n",
    "            f\"{target_info}\",\n",
    "            \"\",\n",
    "            \"### Task\",\n",
    "            \"Using **only** the information above, assess whether the lead investor’s past experience \",\n",
    "            \"suggests that the target company is likely to achieve a successful second fund-raise, IPO, \",\n",
    "            \"or acquisition **within the next 12 months**.\",\n",
    "            \"\",\n",
    "            \"Please discuss:\",\n",
    "            \"• Positive signals from the investor’s track record\",\n",
    "            \"• Potential red flags or weaknesses\",\n",
    "            \"• Any open questions or uncertainties\",\n",
    "            \"\",\n",
    "            \"### Output format (exactly)\",\n",
    "            \"Analysis:\",\n",
    "            \"<your analysis here>\",\n",
    "            \"Prediction: True/False\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "company_prompts = {}\n",
    "for cid, meta in company_map.items():\n",
    "    investor_id = meta.get(\"invest_person\")\n",
    "    tgt_time    = int(meta.get(\"time\", -1))\n",
    "    tgt_info, _ = fetch_basic(cid)\n",
    "\n",
    "    company_prompts[cid] = {\"input_prompt\": \"\"}\n",
    "\n",
    "    if tgt_time < 0 or (not investor_id) or (not tgt_info):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        prompt = build_prompt(investor_id, cid, tgt_time, tgt_info)\n",
    "        company_prompts[cid][\"input_prompt\"] = prompt         \n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] skip {cid} ({investor_id}): {e}\", file=sys.stderr)\n",
    "\n",
    "print(f\"\\nTotal companies: {len(company_prompts)}  \"\n",
    "      f\"Non-empty prompts: {sum(bool(v['input_prompt']) for v in company_prompts.values())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （3） Here we build a prompt based on the graph reasoning path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pickle\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "paths_json = Path(\"../test_data_sampled_path.json\")         \n",
    "train_pkl  = Path(\"../train_data_2022_basic.pkl\")\n",
    "test_pkl   = Path(\"../test_data_2022_basic.pkl\")\n",
    "graph_pkl  = Path(\"../graph_2022.pkl\")\n",
    "out_pkl    = Path(\"../test_data_sampled_prompts_merged.pkl\")\n",
    "\n",
    "company_paths: Dict[str, Dict[str, List[str]]] = json.loads(\n",
    "    paths_json.read_text(encoding=\"utf-8\")\n",
    ")\n",
    "\n",
    "with train_pkl.open(\"rb\") as f:\n",
    "    train_data = pickle.load(f)\n",
    "with test_pkl.open(\"rb\") as f:\n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "basic_info_db = {**train_data, **test_data}\n",
    "\n",
    "with graph_pkl.open(\"rb\") as f:\n",
    "    G: nx.MultiDiGraph = pickle.load(f)\n",
    "\n",
    "id2node = {str(d[\"id\"]): n for n, d in G.nodes(data=True)}\n",
    "\n",
    "def is_person_id(rid: str) -> bool:\n",
    "    return isinstance(rid, str) and rid.endswith(\"P\")\n",
    "\n",
    "def label_str(lbl: Optional[int]) -> str:\n",
    "    return \"True\" if lbl == 1 else (\"False\" if lbl == 0 else \"Unknown\")\n",
    "\n",
    "def company_display_name(cid: str) -> str:\n",
    "    rec = basic_info_db.get(cid) or {}\n",
    "    return (rec.get(\"name\") or \"\").strip() or cid\n",
    "\n",
    "def company_profile_and_label(cid: str) -> Tuple[str, str]:\n",
    "    rec = basic_info_db.get(cid) or {}\n",
    "    profile = (rec.get(\"basic_info\") or \"\").strip()\n",
    "    return profile, label_str(rec.get(\"label\"))\n",
    "\n",
    "def person_profile(pid: str) -> str:\n",
    "    nd = id2node.get(pid)\n",
    "    if nd is None:\n",
    "        return f\"Investor {pid}\"\n",
    "    attrs = G.nodes[nd]\n",
    "    name = (attrs.get(\"name\") or \"\").strip()\n",
    "    binfo = (attrs.get(\"basic_info\") or \"\").strip()\n",
    "    if name and binfo:\n",
    "        return f\"{name} — {binfo}\"\n",
    "    if name:\n",
    "        return name\n",
    "    if binfo:\n",
    "        return binfo\n",
    "    return f\"Investor {pid}\"\n",
    "\n",
    "def format_path(ids: Optional[List[str]]) -> str:\n",
    "    if not ids:\n",
    "        return \"(none)\"\n",
    "    return \" -> \".join(ids)\n",
    "\n",
    "def collect_entities_both_paths(target_cid: str, p1: Optional[List[str]], p2: Optional[List[str]]):\n",
    "    seq = (p1 or []) + (p2 or [])\n",
    "    comp_ids, person_ids = [], []\n",
    "    comp_seen, pers_seen = set(), set()\n",
    "    for rid in seq:\n",
    "        if is_person_id(rid):\n",
    "            if rid not in pers_seen:\n",
    "                pers_seen.add(rid)\n",
    "                person_ids.append(rid)\n",
    "        else:\n",
    "            if rid != target_cid and rid not in comp_seen:\n",
    "                comp_seen.add(rid)\n",
    "                comp_ids.append(rid)\n",
    "    return comp_ids, person_ids\n",
    "\n",
    "def build_merged_prompt(target_cid: str, path1: Optional[List[str]], path2: Optional[List[str]]) -> Optional[str]:\n",
    "    if not (path1 or path2):\n",
    "        return None\n",
    "\n",
    "    target_name = company_display_name(target_cid)\n",
    "    target_profile, _ = company_profile_and_label(target_cid)\n",
    "\n",
    "    path_a = format_path(path1)\n",
    "    path_b = format_path(path2)\n",
    "\n",
    "    comp_ids, person_ids = collect_entities_both_paths(target_cid, path1, path2)\n",
    "\n",
    "    company_profile_lines = []\n",
    "    label_pairs = []\n",
    "    for cid in comp_ids:\n",
    "        info, lbl = company_profile_and_label(cid)\n",
    "        name = company_display_name(cid)\n",
    "        if info:\n",
    "            company_profile_lines.append(f\"  • {name} ({cid}): {info} (Outcome Label: {lbl})\")\n",
    "        else:\n",
    "            company_profile_lines.append(f\"  • {name} ({cid}) (Outcome Label: {lbl})\")\n",
    "        label_pairs.append(f\"{cid}:{lbl}\")\n",
    "    labels_summary = \", \".join(label_pairs) if label_pairs else \"(none)\"\n",
    "\n",
    "    investor_lines = [f\"  • {person_profile(pid)} ({pid})\" for pid in person_ids]\n",
    "\n",
    "    tgt_block = (target_profile or f\"(No profile text available for {target_name})\").strip()\n",
    "\n",
    "    if path_a == \"(none)\" and path_b == \"(none)\":\n",
    "        return None\n",
    "\n",
    "    lines = [\n",
    "        \"Role: You are a senior venture-capital analyst who excels at step-by-step reasoning over investment paths to judge whether a seed/angel-stage start-up is likely to secure Series-A funding within the next year.\",\n",
    "        \"\",\n",
    "        \"You are given the following information blocks:\",\n",
    "        f\"(1) High-value investment paths retrieved for {target_name} ({target_cid}):\",\n",
    "        f\"    (A) {path_a}\",\n",
    "        f\"    (B) {path_b}\",\n",
    "        \"(2) Company profiles appearing in the paths (each with outcome labels; True = raised Series A within 12 months after seed/angel, False = did not):\",\n",
    "        *(company_profile_lines or [\"  (none)\"]),\n",
    "        f\"    Success/Failure: {labels_summary}\",\n",
    "        \"(3) Investor profiles appearing in the paths:\",\n",
    "        *(investor_lines or [\"  (none)\"]),\n",
    "        \"(4) Target company profile:\",\n",
    "        f\"    {tgt_block}\",\n",
    "        \"\",\n",
    "        \"Task:\",\n",
    "        f\"• Analyse the evidence and predict whether {target_name} will raise a Series-A round within 12 months.\",\n",
    "        \"\",\n",
    "        \"Output exactly in the format:\",\n",
    "        \"Prediction: True/False\",\n",
    "        \"Analysis: <your step-by-step reasoning>\",\n",
    "        \"\",\n",
    "        \"If evidence is insufficient, reason cautiously but still decide.\",\n",
    "    ]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "out: Dict[str, Dict[str, object]] = {}\n",
    "skipped = 0\n",
    "\n",
    "for cid, obj in company_paths.items():\n",
    "    p1 = obj.get(\"path1\") or None\n",
    "    p2 = obj.get(\"path2\") or None\n",
    "\n",
    "    prompt = build_merged_prompt(cid, p1, p2)\n",
    "    if not prompt:\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    out[cid] = {\n",
    "        \"paths\": {\"path1\": p1, \"path2\": p2},\n",
    "        \"input_prompt\": prompt,\n",
    "    }\n",
    "\n",
    "print(f\"Built merged prompts for {len(out)} companies; skipped {skipped} without usable content.\")\n",
    "\n",
    "out_pkl.parent.mkdir(parents=True, exist_ok=True)\n",
    "with out_pkl.open(\"wb\") as f:\n",
    "    pickle.dump(out, f)\n",
    "\n",
    "print(\"Saved →\", out_pkl.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Here is the parallel call LLM analysis prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os, pickle, json\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "IN_PKLS = [\n",
    "    Path('../text_similar_company_prompt.pkl'),\n",
    "    Path('../company_prompts_single_investor.pkl'),\n",
    "    Path('../test_data_sampled_prompts_merged.pkl'),\n",
    "]\n",
    "\n",
    "def out_path(in_path: Path) -> Path:\n",
    "    return in_path.with_name(in_path.stem + \"_with_pred.pkl\")\n",
    "\n",
    "BASE_URL = os.getenv(\"BASE_URL\", \"\")\n",
    "MODEL    = \"\"\n",
    "\n",
    "api_keys = [\n",
    "    os.getenv(\"OPENAI_API_KEY\"),\n",
    "    os.getenv(\"OPENAI_API_KEY_2\"),\n",
    "    os.getenv(\"OPENAI_API_KEY_3\"),\n",
    "    os.getenv(\"OPENAI_API_KEY_4\"),\n",
    "    os.getenv(\"OPENAI_API_KEY_5\"),\n",
    "    os.getenv(\"OPENAI_API_KEY_6\"),\n",
    "    os.getenv(\"OPENAI_API_KEY_7\"),\n",
    "    os.getenv(\"OPENAI_API_KEY_8\"),\n",
    "]\n",
    "api_keys = [k for k in api_keys if k]\n",
    "\n",
    "clients = [OpenAI(api_key=k, base_url=BASE_URL) for k in api_keys for _ in range(1)]\n",
    "MAX_WORKERS = 4\n",
    "\n",
    "def call_llm(prompt: str, client: OpenAI) -> str:\n",
    "    resp = client.chat.completions.create(\n",
    "        model       = MODEL,\n",
    "        messages    = [{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature = 0.0,\n",
    "        \n",
    "    )\n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "SAVE_EVERY_N = 50       \n",
    "\n",
    "def process_pkl(in_pkl: Path):\n",
    "    out_pkl = out_path(in_pkl)\n",
    "    with in_pkl.open('rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    if out_pkl.exists():\n",
    "        with out_pkl.open('rb') as f:\n",
    "            saved = pickle.load(f)\n",
    "        for k, v in saved.items():\n",
    "            if isinstance(v, dict) and v.get('prediction'):\n",
    "                data.setdefault(k, {}).update({'prediction': v['prediction']})\n",
    "\n",
    "    tasks, skipped_empty = [], 0\n",
    "    for i, (cid, rec) in enumerate(data.items()):\n",
    "        if rec.get('prediction'):    \n",
    "            continue\n",
    "        prompt = (rec.get('input_prompt') or \"\").strip()\n",
    "        if not prompt:                         \n",
    "            data[cid]['prediction'] = \"\"     \n",
    "            skipped_empty += 1\n",
    "            continue\n",
    "        client = clients[i % MAX_WORKERS]\n",
    "        tasks.append((cid, prompt, client))\n",
    "\n",
    "    print(f\"\\n[{in_pkl.name}] empty prompt skipped: {skipped_empty} | to infer: {len(tasks)}\")\n",
    "\n",
    "    if not tasks:\n",
    "        with out_pkl.open('wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "        return\n",
    "\n",
    "    pending = tasks[:]\n",
    "    completed = 0\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as exe:\n",
    "        futures = {exe.submit(call_llm, p, c): cid for cid, p, c in pending}\n",
    "        pbar = tqdm(total=len(futures), desc=f\"LLM predicting ({in_pkl.name})\")\n",
    "        for fut in as_completed(futures):\n",
    "            cid = futures[fut]\n",
    "            try:\n",
    "                data[cid]['prediction'] = fut.result()\n",
    "            except Exception as e:\n",
    "                print(f\"[Error] {cid}: {e}\")\n",
    "                data[cid]['prediction'] = \"\"\n",
    "            completed += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "            if completed % SAVE_EVERY_N == 0:\n",
    "                with out_pkl.open('wb') as f:\n",
    "                    pickle.dump(data, f)\n",
    "        pbar.close()\n",
    "\n",
    "    with out_pkl.open('wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"[Done] {in_pkl.name} → {out_pkl}\")\n",
    "\n",
    "for pkl_path in IN_PKLS:\n",
    "    if not pkl_path.exists():\n",
    "        print(f\"[WARN] {pkl_path} \")\n",
    "        continue\n",
    "    process_pkl(pkl_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Below is the code to vectorize the output of LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import pickle, numpy as np, re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "root = Path('../3_agent_prompt')\n",
    "IN_PKL  = root / 'combined_predictions.pkl'\n",
    "EMB_NPZ = root / 'processed_vector/MiniLM_text_embed.npz'\n",
    "LBL_NPY = root / 'processed_vector/labels.npy'\n",
    "\n",
    "VIEW_KEYS  = ['c', 'cc', 'pp']\n",
    "MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "raw = pickle.loads(IN_PKL.read_bytes())\n",
    "company_ids = sorted(raw.keys())\n",
    "N = len(company_ids)\n",
    "print(f\"Total samples: {N}\")\n",
    "\n",
    "empty_mask = np.zeros((N, 4), dtype=bool) \n",
    "texts  = []\n",
    "labels = np.full((N, 4), fill_value=-1, dtype=np.int8) \n",
    "\n",
    "for i, cid in enumerate(company_ids):\n",
    "    rec = raw[cid]\n",
    "    for j, vk in enumerate(VIEW_KEYS):\n",
    "        view = rec.get(vk, {}) or {}                      \n",
    "        txt  = (view.get('text') or '').strip()\n",
    "        lbl  = view.get('label')\n",
    "\n",
    "        texts.append(txt)\n",
    "        if lbl in (0, 1):             \n",
    "            labels[i, j] = lbl\n",
    "        if txt == '':\n",
    "            empty_mask[i, j] = True    \n",
    "\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "emb = model.encode(\n",
    "        texts,\n",
    "        batch_size=64,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True,\n",
    "        show_progress_bar=True\n",
    ")                                 \n",
    "\n",
    "vectors_text = emb.reshape(N, 4, -1).astype('float32')\n",
    "\n",
    "vectors_text[empty_mask] = 0.0\n",
    "\n",
    "print(\"vectors_text shape :\", vectors_text.shape)\n",
    "print(\"labels shape       :\", labels.shape)\n",
    "print(f\"Empty texts count  : {empty_mask.sum()}\")\n",
    "\n",
    "EMB_NPZ.parent.mkdir(parents=True, exist_ok=True)\n",
    "np.savez_compressed(\n",
    "    EMB_NPZ,\n",
    "    company_ids=np.array(company_ids, dtype='<U20'),\n",
    "    view_keys=np.array(VIEW_KEYS),\n",
    "    vectors_text=vectors_text\n",
    ")\n",
    "np.save(LBL_NPY, labels)\n",
    "\n",
    "print(f\"Saved embeddings → {EMB_NPZ}\")\n",
    "print(f\"Saved labels     → {LBL_NPY}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Here is the code that handles the vectorization of the test company features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd, numpy as np, json, torch\n",
    "from pathlib import Path\n",
    "\n",
    "ID2JSON   = Path('../ID2index.json')\n",
    "COMP_CSV  = Path('../test_data_sampled.csv')\n",
    "COMP_RAW  = Path('../Company.csv')\n",
    "DEAL_RAW  = Path('../Deal.csv')\n",
    "\n",
    "INDUSTRY_DIM = 39            \n",
    "\n",
    "ID2index   = json.loads(ID2JSON.read_text())\n",
    "target_ids = pd.read_csv(COMP_CSV, dtype=str)['CompanyID'].tolist()\n",
    "target_ids = [cid for cid in target_ids if cid in ID2index]\n",
    "print(f\"Target companies in mapping: {len(target_ids)}\")\n",
    "\n",
    "company_df = pd.read_csv(COMP_RAW)\n",
    "deal_df    = pd.read_csv(DEAL_RAW)\n",
    "company_df = company_df[company_df['CompanyID'].isin(target_ids)]\n",
    "deal_df    = deal_df[(deal_df['CompanyID'].isin(target_ids)) & (deal_df['DealNo'] == 1)]\n",
    "\n",
    "company_df['Index'] = company_df['CompanyID'].map(ID2index)\n",
    "deal_df['Index']    = deal_df['CompanyID'].map(ID2index)\n",
    "company_df = company_df.set_index('Index', drop=False)\n",
    "deal_df    = deal_df.set_index('Index', drop=False)\n",
    "\n",
    "all_industries = sorted(company_df['PrimaryIndustryGroup'].dropna().unique())\n",
    "industry_list  = all_industries[:INDUSTRY_DIM]          \n",
    "Industry2idx   = {name: i for i, name in enumerate(industry_list)}\n",
    "\n",
    "dealtype_list  = sorted(deal_df['DealType'].dropna().unique())\n",
    "DealType2idx   = {name: i for i, name in enumerate(dealtype_list)}\n",
    "\n",
    "print(f\"Industry categories (capped) : {len(industry_list)}/{INDUSTRY_DIM}\")\n",
    "print(f\"DealType categories          : {len(DealType2idx)}\")\n",
    "\n",
    "dim = 2 + INDUSTRY_DIM + len(DealType2idx)          \n",
    "mat = torch.zeros(len(ID2index), dim, dtype=torch.float32)\n",
    "\n",
    "for idx, row in company_df.iterrows():\n",
    "    mat[idx, 0] = row['YearFounded'] if not pd.isna(row['YearFounded']) else 0\n",
    "    ind = row['PrimaryIndustryGroup']\n",
    "    if ind in Industry2idx:                          \n",
    "        mat[idx, 2 + Industry2idx[ind]] = 1\n",
    "\n",
    "base_dt = 2 + INDUSTRY_DIM\n",
    "for idx, row in deal_df.iterrows():\n",
    "    mat[idx, 1] = row['DealSize'] if not pd.isna(row['DealSize']) else 0\n",
    "    dt = row['DealType']\n",
    "    if dt in DealType2idx:\n",
    "        mat[idx, base_dt + DealType2idx[dt]] = 1\n",
    "\n",
    "row_idx = [ID2index[cid] for cid in target_ids]\n",
    "attr_matrix = mat[row_idx].numpy().astype('float32')\n",
    "np.save('test_sample_company_attr_target.npy', attr_matrix)\n",
    "np.save('test_sample_company_id_order.npy', np.array(target_ids))\n",
    "print(\"Saved attr matrix :\", attr_matrix.shape)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Here is the code to get the weight of the corresponding agent through the output of the three agents and the company feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "root = Path(\"..\")\n",
    "EMB_NPZ = root / \"processed_vector\" / \"MiniLM_text_embed.npz\"\n",
    "LBL_NPY = root / \"processed_vector\" / \"labels.npy\"\n",
    "TEST_CSV = root / \"test_data_sampled.csv\"\n",
    "\n",
    "ATTR_NPY = root / \"test_sample_company_attr_target.npy\"\n",
    "ATTR_ID_ORDER_OPT = root / \"test_sample_company_id_order.npy\"   \n",
    "\n",
    "MODEL_CKPT = Path(\"./best_gate.pt\")   \n",
    "OUT_CSV = root / \"test_view_weights.csv\"\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "npz = np.load(EMB_NPZ, allow_pickle=True)\n",
    "company_ids_all = npz[\"company_ids\"]            \n",
    "V_text_all = npz[\"vectors_text\"].astype(np.float32)  \n",
    "\n",
    "bool_raw = np.load(LBL_NPY).astype(np.float32)  \n",
    "bool_all = bool_raw[..., None]                 \n",
    "\n",
    "N, n_views, D_text = V_text_all.shape\n",
    "assert n_views == 3, f\"Expect 3 views, got {n_views}\"\n",
    "\n",
    "id2idx = {str(cid): i for i, cid in enumerate(company_ids_all)}\n",
    "\n",
    "df_test = pd.read_csv(TEST_CSV, dtype={\"CompanyID\": str})\n",
    "test_ids = df_test[\"CompanyID\"].astype(str).tolist()\n",
    "\n",
    "kept = [cid for cid in test_ids if cid in id2idx]\n",
    "missing = [cid for cid in test_ids if cid not in id2idx]\n",
    "if missing:\n",
    "    print(f\"[WARN] {len(missing)} test CompanyID(s) not in embeddings; will skip.\")\n",
    "\n",
    "idxs = [id2idx[cid] for cid in kept]\n",
    "\n",
    "V_text = V_text_all[idxs]    \n",
    "B_bool = bool_all[idxs]       \n",
    "cids_sub = np.array(kept)\n",
    "\n",
    "attr_mat = np.load(ATTR_NPY)  \n",
    "K = attr_mat.shape[1]\n",
    "\n",
    "if ATTR_ID_ORDER_OPT.exists():\n",
    "    attr_ids = np.load(ATTR_ID_ORDER_OPT)\n",
    "    id2attr = {str(cid): i for i, cid in enumerate(attr_ids)}\n",
    "    attr_rows = []\n",
    "    not_found_attr = []\n",
    "    for cid in cids_sub:\n",
    "        j = id2attr.get(cid)\n",
    "        if j is None:\n",
    "            not_found_attr.append(cid)\n",
    "            attr_rows.append(np.zeros((K,), dtype=np.float32))\n",
    "        else:\n",
    "            attr_rows.append(attr_mat[j])\n",
    "    if not_found_attr:\n",
    "        print(f\"[WARN] {len(not_found_attr)} companies missing in ATTR id order; filled zeros.\")\n",
    "    C_feat = np.stack(attr_rows, axis=0).astype(np.float32)\n",
    "else:\n",
    "    if attr_mat.shape[0] != len(cids_sub):\n",
    "        print(f\"[WARN] ATTR rows ({attr_mat.shape[0]}) != #test companies ({len(cids_sub)}). \"\n",
    "              f\"Will truncate/pad with zeros as needed.\")\n",
    "    if attr_mat.shape[0] >= len(cids_sub):\n",
    "        C_feat = attr_mat[:len(cids_sub)].astype(np.float32)\n",
    "    else:\n",
    "        pad = np.zeros((len(cids_sub) - attr_mat.shape[0], K), dtype=np.float32)\n",
    "        C_feat = np.vstack([attr_mat, pad]).astype(np.float32)\n",
    "\n",
    "class MoEGate(nn.Module):\n",
    "    def __init__(self, text_dim=384, comp_dim=1, n_views=3):\n",
    "        super().__init__()\n",
    "        self.n_views = n_views\n",
    "        self.log_wb = nn.Parameter(torch.tensor(1.1)) \n",
    "\n",
    "        view_dim = text_dim + 1  \n",
    "\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(n_views * view_dim + comp_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_views),\n",
    "        )\n",
    "        self.clf = nn.Sequential(\n",
    "            nn.Linear(view_dim, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, V_text, B_bool, C):\n",
    "        wb = torch.exp(self.log_wb)\n",
    "        V = torch.cat([V_text, wb * B_bool], dim=-1)   \n",
    "        B = V.size(0)\n",
    "        flat = torch.cat([V.view(B, -1), C], dim=-1)    \n",
    "        alpha = torch.softmax(self.gate(flat), dim=-1) \n",
    "        v_agg = (alpha.unsqueeze(-1) * V).sum(1)         \n",
    "        logit = self.clf(v_agg).squeeze(-1)             \n",
    "        return logit, alpha\n",
    "\n",
    "model = MoEGate(text_dim=D_text, comp_dim=K, n_views=3).to(DEVICE)\n",
    "state = torch.load(MODEL_CKPT, map_location=DEVICE)\n",
    "model.load_state_dict(state)\n",
    "model.eval()\n",
    "\n",
    "class InferDS(Dataset):\n",
    "    def __init__(self, V, B, C, ids):\n",
    "        self.V = torch.from_numpy(V).float()\n",
    "        self.B = torch.from_numpy(B).float()\n",
    "        self.C = torch.from_numpy(C).float()\n",
    "        self.ids = list(ids)\n",
    "    def __len__(self): return len(self.ids)\n",
    "    def __getitem__(self, i):\n",
    "        return self.V[i], self.B[i], self.C[i], self.ids[i]\n",
    "\n",
    "loader = DataLoader(InferDS(V_text, B_bool, C_feat, cids_sub),\n",
    "                    batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "rows = [] \n",
    "with torch.no_grad():\n",
    "    for Vb, Bb, Cb, id_batch in loader:\n",
    "        Vb, Bb, Cb = Vb.to(DEVICE), Bb.to(DEVICE), Cb.to(DEVICE)\n",
    "        logits, alpha = model(Vb, Bb, Cb) \n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        w = alpha.cpu().numpy()\n",
    "        for cid, p, wi in zip(id_batch, probs, w):\n",
    "            rows.append({\n",
    "                \"CompanyID\": cid,\n",
    "                \"w1\": float(wi[0]),\n",
    "                \"w2\": float(wi[1]),\n",
    "                \"w3\": float(wi[2]),\n",
    "                \"prob\": float(p),\n",
    "            })\n",
    "\n",
    "df_out = pd.DataFrame(rows, columns=[\"CompanyID\", \"w1\", \"w2\", \"w3\", \"prob\"])\n",
    "OUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_out.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
    "print(f\"Saved gating weights → {OUT_CSV.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Here is the process of constructing the prompt of Manager Agent based on the output of the three agents and the weights for each sample obtained above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from pathlib import Path\n",
    "import pickle, json, re\n",
    "from typing import Dict, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "ROOT = Path(\"..\")\n",
    "\n",
    "IN_PKLS = [\n",
    "     \"../text_similar_company_prompt.pkl\",   \n",
    "     \"../company_prompts_single_investor.pkl\",  \n",
    "     \"../test_data_sampled_prompts_merged.pkl\", \n",
    "]\n",
    "\n",
    "WEIGHT_CSV = \"../test_view_weights.csv\"\n",
    "\n",
    "TRAIN_BASIC_PKL = \"../train_data_2022_basic.pkl\"\n",
    "TEST_BASIC_PKL  = \"../test_data_2022_basic.pkl\"\n",
    "\n",
    "OUT_PKL = \"../manager_prompts.pkl\"\n",
    "\n",
    "def norm_pred(x: Any) -> str:\n",
    "    if x is None: return \"Unknown\"\n",
    "    s = str(x).strip().lower()\n",
    "    if s in {\"1\",\"true\",\"yes\",\"y\",\"positive\"}: return \"True\"\n",
    "    if s in {\"0\",\"false\",\"no\",\"n\",\"negative\"}: return \"False\"\n",
    "    m = re.search(r\"\\b(prediction)\\s*:\\s*(true|false)\\b\", s, re.I)\n",
    "    if m: return m.group(2).capitalize()\n",
    "    return \"Unknown\"\n",
    "\n",
    "PRED_RE = re.compile(r\"(?im)^\\s*prediction\\s*:\\s*(true|false)\\s*$\")\n",
    "ANAL_RE = re.compile(r\"(?is)\\banalysis\\s*:\\s*(.+)\\Z\")\n",
    "\n",
    "def parse_pred_analysis_from_text(txt: str) -> Dict[str,str]:\n",
    "    \"\"\"从一段 LLM 文本里提取 Prediction/Analysis。\"\"\"\n",
    "    pred = \"Unknown\"; ana = \"\"\n",
    "    if not txt:\n",
    "        return {\"prediction\": pred, \"analysis\": ana}\n",
    "    m1 = PRED_RE.search(txt)\n",
    "    if m1: pred = m1.group(1).capitalize()\n",
    "    m2 = ANAL_RE.search(txt)\n",
    "    if m2: ana = m2.group(1).strip()\n",
    "    return {\"prediction\": pred, \"analysis\": ana}\n",
    "\n",
    "def lower_keys(d: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    return { (k.lower() if isinstance(k,str) else k): v for k,v in d.items() }\n",
    "\n",
    "def load_agent_pkl(pkl_path: Path) -> Dict[str, Dict[str, str]]:\n",
    "    data = pickle.loads(pkl_path.read_bytes())\n",
    "    out: Dict[str, Dict[str,str]] = {}\n",
    "    if not isinstance(data, dict):\n",
    "        raise ValueError(f\"{pkl_path} should be a dict keyed by CompanyID\")\n",
    "\n",
    "    for cid, v in data.items():\n",
    "        cid_str = str(cid)\n",
    "        if not isinstance(v, dict):\n",
    "            if isinstance(v, str):\n",
    "                pa = parse_pred_analysis_from_text(v)\n",
    "                out[cid_str] = pa\n",
    "            else:\n",
    "                out[cid_str] = {\"prediction\": \"Unknown\", \"analysis\": \"\"}\n",
    "            continue\n",
    "\n",
    "        lv = lower_keys(v)\n",
    "        pred = lv.get(\"prediction\")\n",
    "        ana  = lv.get(\"analysis\")\n",
    "\n",
    "        if pred is not None or ana is not None:\n",
    "            out[cid_str] = {\n",
    "                \"prediction\": norm_pred(pred),\n",
    "                \"analysis\": (\"\" if ana is None else str(ana)).strip()\n",
    "            }\n",
    "            continue\n",
    "\n",
    "        for key in [\"output\"]:\n",
    "            if key in lv and isinstance(lv[key], str):\n",
    "                pa = parse_pred_analysis_from_text(lv[key])\n",
    "                out[cid_str] = pa\n",
    "                break\n",
    "        else:\n",
    "            out[cid_str] = {\"prediction\": \"Unknown\", \"analysis\": \"\"}\n",
    "\n",
    "    return out\n",
    "\n",
    "def load_basic_db() -> Dict[str, Dict[str, Any]]:\n",
    "    with TRAIN_BASIC_PKL.open(\"rb\") as f:\n",
    "        train_db = pickle.load(f)\n",
    "    with TEST_BASIC_PKL.open(\"rb\") as f:\n",
    "        test_db = pickle.load(f)\n",
    "    return {**train_db, **test_db}\n",
    "\n",
    "def company_profile(db: Dict[str, Dict[str, Any]], cid: str) -> str:\n",
    "    rec = db.get(cid, {})\n",
    "    return (rec.get(\"basic_info\") or \"\").strip()\n",
    "\n",
    "def company_name(db: Dict[str, Dict[str, Any]], cid: str) -> str:\n",
    "    rec = db.get(cid, {})\n",
    "    return (rec.get(\"name\") or \"\").strip() or cid\n",
    "\n",
    "wdf = pd.read_csv(WEIGHT_CSV, dtype={\"CompanyID\": str})\n",
    "cols = {c.lower(): c for c in wdf.columns}\n",
    "col_cid = cols.get(\"companyid\",\"CompanyID\")\n",
    "w1c = cols.get(\"w1\",\"w1\"); w2c = cols.get(\"w2\",\"w2\"); w3c = cols.get(\"w3\",\"w3\")\n",
    "\n",
    "def normalize_row(row):\n",
    "    w = np.array([row[w1c], row[w2c], row[w3c]], dtype=float)\n",
    "    s = w.sum()\n",
    "    return (w/s if s>0 else np.array([1/3,1/3,1/3], float)).astype(float)\n",
    "\n",
    "weights_map = { str(r[col_cid]): normalize_row(r) for _, r in wdf.iterrows() }\n",
    "\n",
    "sim_map  = load_agent_pkl(IN_PKLS[0])   \n",
    "inv_map  = load_agent_pkl(IN_PKLS[1])  \n",
    "path_map = load_agent_pkl(IN_PKLS[2])  \n",
    "\n",
    "basic_db = load_basic_db()\n",
    "\n",
    "def build_manager_prompt(cid: str, w_path: float, w_sim: float, w_inv: float) -> str:\n",
    "    path_pred = path_map.get(cid, {}).get(\"prediction\",\"Unknown\")\n",
    "    path_ana  = path_map.get(cid, {}).get(\"analysis\",\"\")\n",
    "\n",
    "    sim_pred  = sim_map.get(cid, {}).get(\"prediction\",\"Unknown\")\n",
    "    sim_ana   = sim_map.get(cid, {}).get(\"analysis\",\"\")\n",
    "\n",
    "    inv_pred  = inv_map.get(cid, {}).get(\"prediction\",\"Unknown\")\n",
    "    inv_ana   = inv_map.get(cid, {}).get(\"analysis\",\"\")\n",
    "\n",
    "    tgt_name  = company_name(basic_db, cid)\n",
    "    tgt_prof  = company_profile(basic_db, cid) or f\"(No profile text available for {tgt_name})\"\n",
    "\n",
    "    weights_text = f\"[Path={w_path:.3f}, Similar-company={w_sim:.3f}, Lead-investor={w_inv:.3f}]\"\n",
    "\n",
    "    lines = [\n",
    "        \"Role:\",\n",
    "        \"You are a senior venture-capital analyst who excels at synthesizing other experts' viewpoints to decide whether a seed/angel-stage start-up will secure Series-A funding within the next year.\",\n",
    "        \"\",\n",
    "        \"You are given:\",\n",
    "        \"\",\n",
    "        \"(1) Path-analyst verdict\",\n",
    "        f\"• Prediction: {path_pred}\",\n",
    "        f\"• Analysis  : {path_ana}\",\n",
    "        \"\",\n",
    "        \"(2) Similar-company analyst verdict\",\n",
    "        f\"• Prediction: {sim_pred}\",\n",
    "        f\"• Analysis  : {sim_ana}\",\n",
    "        \"\",\n",
    "        \"(3) Lead-investor analyst verdict\",\n",
    "        f\"• Prediction: {inv_pred}\",\n",
    "        f\"• Analysis  : {inv_ana}\",\n",
    "        \"\",\n",
    "        \"(4) Aggregate-weight advice\",\n",
    "        \"The historical importance of the three perspectives is\",\n",
    "        f\"{weights_text}\",\n",
    "        \"\",\n",
    "        \"(5) Target company profile\",\n",
    "        f\"{tgt_prof}\",\n",
    "        \"\",\n",
    "        \"Task:\",\n",
    "        \"• Produce a single, final prediction on whether the target will raise a Series-A round within 12 months.\",\n",
    "        \"• Output **exactly** in the format:\",\n",
    "        \"\",\n",
    "        \"Prediction: True/False\",\n",
    "        \"Analysis: <your step-by-step reasoning>\",\n",
    "        \"\",\n",
    "        \"• If evidence is insufficient, reason cautiously but still decide.\",\n",
    "    ]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "out = {}\n",
    "skipped = 0\n",
    "\n",
    "for cid, w_vec in weights_map.items():\n",
    "    w_path, w_sim, w_inv = float(w_vec[0]), float(w_vec[1]), float(w_vec[2])\n",
    "\n",
    "    try:\n",
    "        prompt = build_manager_prompt(cid, w_path, w_sim, w_inv)\n",
    "        out[cid] = {\n",
    "            \"input_prompt\": prompt,\n",
    "            \"weights\": [w_path, w_sim, w_inv],\n",
    "            \"meta\": {\n",
    "                \"path_pred\": path_map.get(cid, {}).get(\"prediction\", \"Unknown\"),\n",
    "                \"path_analysis\": path_map.get(cid, {}).get(\"analysis\", \"\"),\n",
    "                \"sim_pred\":  sim_map.get(cid, {}).get(\"prediction\", \"Unknown\"),\n",
    "                \"sim_analysis\": sim_map.get(cid, {}).get(\"analysis\", \"\"),\n",
    "                \"inv_pred\":  inv_map.get(cid, {}).get(\"prediction\", \"Unknown\"),\n",
    "                \"inv_analysis\": inv_map.get(cid, {}).get(\"analysis\", \"\"),\n",
    "                \"target_profile\": company_profile(basic_db, cid),\n",
    "            }\n",
    "        }\n",
    "    except Exception as e:\n",
    "        skipped += 1\n",
    "        print(f\"[WARN] skip {cid}: {e}\")\n",
    "\n",
    "print(f\"Built manager prompts: {len(out)}  (skipped {skipped})\")\n",
    "\n",
    "OUT_PKL.parent.mkdir(parents=True, exist_ok=True)\n",
    "with OUT_PKL.open(\"wb\") as f:\n",
    "    pickle.dump(out, f)\n",
    "\n",
    "print(\"Saved →\", OUT_PKL.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Here is the process of parallel reasoning to achieve the final Manager Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "IN_PKL  = Path(\"../manager_prompts.pkl\")\n",
    "OUT_PKL = IN_PKL.with_name(IN_PKL.stem + \"_with_pred.pkl\")\n",
    "\n",
    "BASE_URL = os.getenv(\"BASE_URL\", \"\")   \n",
    "MODEL    = os.getenv(\"LLM_MODEL\", \"\")  \n",
    "\n",
    "api_keys = [\n",
    "    os.getenv(\"OPENAI_API_KEY\"),\n",
    "    os.getenv(\"OPENAI_API_KEY_2\"),\n",
    "    os.getenv(\"OPENAI_API_KEY_3\"),\n",
    "    os.getenv(\"OPENAI_API_KEY_4\"),\n",
    "    os.getenv(\"OPENAI_API_KEY_5\"),\n",
    "    os.getenv(\"OPENAI_API_KEY_6\"),\n",
    "    os.getenv(\"OPENAI_API_KEY_7\"),\n",
    "    os.getenv(\"OPENAI_API_KEY_8\"),\n",
    "]\n",
    "api_keys = [k for k in api_keys if k]\n",
    "if not api_keys:\n",
    "    raise RuntimeError(\"No API key found in env (OPENAI_API_KEY, ...).\")\n",
    "\n",
    "clients = [OpenAI(api_key=k, base_url=BASE_URL) for k in api_keys]\n",
    "MAX_WORKERS = min(8, len(clients)) \n",
    "\n",
    "PRED_RE = re.compile(r\"(?im)^\\s*prediction\\s*:\\s*(true|false)\\s*$\")\n",
    "ANAL_RE = re.compile(r\"(?is)\\banalysis\\s*:\\s*(.+)\\Z\")\n",
    "\n",
    "def parse_prediction_analysis(text: str):\n",
    "    \"\"\"Extract `Prediction:` (True/False) and `Analysis:` from model output.\"\"\"\n",
    "    if not text:\n",
    "        return \"Unknown\", \"\"\n",
    "    pred = \"Unknown\"\n",
    "    m1 = PRED_RE.search(text)\n",
    "    if m1:\n",
    "        pred = m1.group(1).capitalize()\n",
    "    m2 = ANAL_RE.search(text)\n",
    "    analysis = m2.group(1).strip() if m2 else \"\"\n",
    "    return pred, analysis\n",
    "\n",
    "def call_llm(prompt: str, client: OpenAI) -> str:\n",
    "    resp = client.chat.completions.create(\n",
    "        model       = MODEL,\n",
    "        messages    = [{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature = 0.0,\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "SAVE_EVERY_N = 50\n",
    "\n",
    "def main():\n",
    "    if not IN_PKL.exists():\n",
    "        raise FileNotFoundError(f\"Input PKL not found: {IN_PKL}\")\n",
    "\n",
    "    with IN_PKL.open(\"rb\") as f:\n",
    "        data = pickle.load(f) \n",
    "\n",
    "    if OUT_PKL.exists():\n",
    "        with OUT_PKL.open(\"rb\") as f:\n",
    "            saved = pickle.load(f)\n",
    "        for cid, rec in saved.items():\n",
    "            if isinstance(rec, dict) and rec.get(\"prediction\"):\n",
    "                data.setdefault(cid, {}).update({\n",
    "                    \"prediction\": rec.get(\"prediction\"),\n",
    "                    \"manager_prediction\": rec.get(\"manager_prediction\"),\n",
    "                    \"manager_analysis\": rec.get(\"manager_analysis\"),\n",
    "                })\n",
    "\n",
    "    tasks = []\n",
    "    skipped_empty = 0\n",
    "    already_done  = 0\n",
    "    for i, (cid, rec) in enumerate(data.items()):\n",
    "        if rec.get(\"prediction\"):        \n",
    "            already_done += 1\n",
    "            continue\n",
    "        prompt = (rec.get(\"input_prompt\") or \"\").strip()\n",
    "        if not prompt:\n",
    "            data[cid][\"prediction\"] = \"\"\n",
    "            data[cid][\"manager_prediction\"] = \"Unknown\"\n",
    "            data[cid][\"manager_analysis\"] = \"\"\n",
    "            skipped_empty += 1\n",
    "            continue\n",
    "        client = clients[i % max(1, len(clients))]\n",
    "        tasks.append((cid, prompt, client))\n",
    "\n",
    "    print(f\"\\n[{IN_PKL.name}] already_done: {already_done} | empty skipped: {skipped_empty} | to infer: {len(tasks)}\")\n",
    "\n",
    "    if tasks:\n",
    "        completed = 0\n",
    "        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as exe:\n",
    "            futures = {exe.submit(call_llm, p, c): cid for cid, p, c in tasks}\n",
    "            pbar = tqdm(total=len(futures), desc=f\"LLM predicting ({IN_PKL.name})\")\n",
    "            for fut in as_completed(futures):\n",
    "                cid = futures[fut]\n",
    "                try:\n",
    "                    raw = fut.result()\n",
    "                    pred, ana = parse_prediction_analysis(raw)\n",
    "                    data[cid][\"prediction\"] = raw                  \n",
    "                    data[cid][\"manager_prediction\"] = pred        \n",
    "                    data[cid][\"manager_analysis\"] = ana           \n",
    "                except Exception as e:\n",
    "                    print(f\"[Error] {cid}: {e}\")\n",
    "                    data[cid][\"prediction\"] = \"\"\n",
    "                    data[cid][\"manager_prediction\"] = \"Unknown\"\n",
    "                    data[cid][\"manager_analysis\"] = \"\"\n",
    "                completed += 1\n",
    "                pbar.update(1)\n",
    "                if completed % SAVE_EVERY_N == 0:\n",
    "                    with OUT_PKL.open(\"wb\") as f:\n",
    "                        pickle.dump(data, f)\n",
    "            pbar.close()\n",
    "\n",
    "    with OUT_PKL.open(\"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"[Done] {IN_PKL.name} → {OUT_PKL}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VC_LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
