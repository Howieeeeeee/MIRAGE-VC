{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. Extract training/validation/test data for the weight generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import math, random, pickle\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "csv_path   = Path('../new_company.csv')\n",
    "graph_pkl  = Path('../graph_2022_invest.pkl')\n",
    "\n",
    "pkl_a      = Path('../test_data_2022_basic.pkl')\n",
    "pkl_b      = Path('../train_data_2022_basic.pkl')\n",
    "\n",
    "out_dir    = csv_path.parent / 'stratified_sampling'\n",
    "out_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "df = pd.read_csv(csv_path, dtype={'time': int, 'CompanyID': str})\n",
    "\n",
    "print('[Info] Loading graph...')\n",
    "G = pickle.loads(graph_pkl.read_bytes())\n",
    "label_map = {d.get('id', n): d.get('label') for n, d in G.nodes(data=True)}\n",
    "\n",
    "df['label'] = df['CompanyID'].map(label_map)\n",
    "df = df.dropna(subset=['label']).reset_index(drop=True)\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "def stratified_sample(group: pd.DataFrame, frac: float, rng: random.Random):\n",
    "    pos_mask = group['label'] == 1\n",
    "    n_total  = len(group)\n",
    "    n_take   = max(1, math.ceil(n_total * frac))\n",
    "\n",
    "    n_pos = pos_mask.sum()\n",
    "    if n_pos in {0, n_total}:\n",
    "        return group.sample(n=min(n_take, n_total),\n",
    "                            random_state=rng.randint(0, 2**32-1))\n",
    "\n",
    "    ratio_pos = n_pos / n_total\n",
    "    n_pos_take = max(1, round(n_take * ratio_pos))\n",
    "    n_neg_take = n_take - n_pos_take\n",
    "\n",
    "    pos_df = group[pos_mask].sample(n=min(n_pos_take, n_pos),\n",
    "                                    random_state=rng.randint(0, 2**32-1))\n",
    "    neg_df = group[~pos_mask].sample(n=min(n_neg_take, len(group)-n_pos),\n",
    "                                     random_state=rng.randint(0, 2**32-1))\n",
    "    return pd.concat([pos_df, neg_df])\n",
    "\n",
    "rng = random.Random(42)\n",
    "sampled_parts = [stratified_sample(g, 0.2, rng) for _, g in df.groupby('time')]\n",
    "sampled_df = (pd.concat(sampled_parts)\n",
    "                .sort_values(['time', 'CompanyID'])\n",
    "                .reset_index(drop=True))\n",
    "\n",
    "sampled_df = sampled_df[sampled_df['label'] != -1]\n",
    "\n",
    "with pkl_a.open('rb') as f:\n",
    "    keys_a = set(pickle.load(f).keys())\n",
    "with pkl_b.open('rb') as f:\n",
    "    keys_b = set(pickle.load(f).keys())\n",
    "\n",
    "valid_ids = keys_a | keys_b        \n",
    "sampled_df = sampled_df[sampled_df['CompanyID'].isin(valid_ids)].reset_index(drop=True)\n",
    "\n",
    "df.to_csv(out_dir / 'new_company_with_label.csv', index=False)\n",
    "sampled_df.to_csv(out_dir / 'sampled_10pct_temporal_stratified.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. For each sample, we construct three perspectives for all information prompts. The three perspectives are: (1) Companies with similar backgrounds; (2) Lead investor background analysis; (3) Graph reasoning path analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Companies with similar backgrounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pickle, json, numpy as np, pandas as pd, torch\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "csv_path  = Path('../sampled_10pct_temporal_stratified.csv')\n",
    "test_pkl  = Path('../test_data_2022_basic.pkl')\n",
    "train_pkl  = Path('../train_data_2022_basic.pkl')\n",
    "out_path  = Path('../test_to_similar.json')\n",
    "\n",
    "df = pd.read_csv(csv_path, dtype={'CompanyID': str})\n",
    "df = df[(df['time'] >= 50) & (df['time'] <= 190)]\n",
    "comp2time = dict(zip(df['CompanyID'], df['time']))\n",
    "\n",
    "query_ids = list(comp2time.keys())\n",
    "print(f\"Total query companies (time 50-190): {len(query_ids)}\")\n",
    "\n",
    "with test_pkl.open('rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "with train_pkl.open('rb') as f:\n",
    "    hist_data = pickle.load(f)\n",
    "\n",
    "all_data = {}\n",
    "all_data.update(hist_data)\n",
    "all_data.update(test_data)         \n",
    "\n",
    "candidate_keys, candidate_texts, candidate_times = [], [], []\n",
    "missing_basic = 0\n",
    "for cid, t in comp2time.items():\n",
    "    info = all_data.get(cid)\n",
    "    if info and 'basic_info' in info and info['basic_info']:\n",
    "        candidate_keys.append(cid)\n",
    "        candidate_texts.append(info['basic_info'])\n",
    "        candidate_times.append(t)\n",
    "    else:\n",
    "        missing_basic += 1\n",
    "\n",
    "print(f\"Candidates w/ basic_info : {len(candidate_keys)}\")\n",
    "if missing_basic:\n",
    "    print(f\"‼  {missing_basic} companies skipped (basic_info missing)\")\n",
    "\n",
    "candidate_times = np.array(candidate_times, dtype=np.int16)\n",
    "\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "model  = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "batch_size = 512\n",
    "cand_embs  = []\n",
    "for i in tqdm(range(0, len(candidate_texts), batch_size), desc='Encoding candidates'):\n",
    "    batch = candidate_texts[i:i+batch_size]\n",
    "    emb   = model.encode(batch,\n",
    "                         batch_size=len(batch),\n",
    "                         convert_to_numpy=True,\n",
    "                         normalize_embeddings=True,\n",
    "                         show_progress_bar=False)\n",
    "    cand_embs.append(emb)\n",
    "cand_embs = np.vstack(cand_embs).astype('float32')   # [N, 384]\n",
    "\n",
    "result = {}\n",
    "for qid in tqdm(query_ids, desc='Searching'):\n",
    "    q_info = all_data.get(qid)\n",
    "    if (not q_info) or ('basic_info' not in q_info):\n",
    "        continue\n",
    "\n",
    "    q_time = comp2time[qid]\n",
    "    q_emb  = model.encode(q_info['basic_info'],\n",
    "                          convert_to_numpy=True,\n",
    "                          normalize_embeddings=True)\n",
    "\n",
    "    mask = candidate_times <= q_time\n",
    "\n",
    "    if qid in candidate_keys:\n",
    "        self_idx = candidate_keys.index(qid)\n",
    "        mask[self_idx] = False\n",
    "\n",
    "    if not mask.any():\n",
    "        continue\n",
    "\n",
    "    sims = cand_embs[mask] @ q_emb          \n",
    "    masked_keys = np.array(candidate_keys)[mask]\n",
    "\n",
    "    top_k = 4 if sims.shape[0] >= 4 else sims.shape[0]\n",
    "    top_idx = np.argpartition(-sims, top_k-1)[:top_k]\n",
    "    top_idx = top_idx[np.argsort(-sims[top_idx])]     \n",
    "\n",
    "    result[qid] = masked_keys[top_idx].tolist()\n",
    "\n",
    "with out_path.open('w', encoding='utf-8') as f:\n",
    "    json.dump(result, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Lead investor background analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pickle, json, networkx as nx, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "strat_csv     = Path('../sampled_10pct_temporal_stratified.csv')\n",
    "time_csv      = Path('../company_time_id.csv')\n",
    "graph_pkl     = Path('../graph_2022.pkl')\n",
    "out_json_path = Path('../test_company_to_investor.json')\n",
    "\n",
    "df_q = pd.read_csv(strat_csv, dtype={'CompanyID': str})\n",
    "df_q = df_q[(df_q['time'] >= 50) & (df_q['time'] <= 190)]\n",
    "query_ids      = set(df_q['CompanyID'])\n",
    "query_time_map = dict(zip(df_q['CompanyID'], df_q['time']))\n",
    "print(f\"Query companies: {len(query_ids)}\")\n",
    "\n",
    "df_time = pd.read_csv(time_csv, dtype={'CompanyID': str}).drop_duplicates(subset=['CompanyID'])\n",
    "time_map = dict(zip(df_time['CompanyID'], df_time['time']))\n",
    "time_map.update(query_time_map)          \n",
    "\n",
    "with graph_pkl.open('rb') as f:\n",
    "    G: nx.MultiGraph = pickle.load(f)\n",
    "\n",
    "# real-id → node-idx\n",
    "id_to_node = {attr['id']: n for n, attr in G.nodes(data=True)}\n",
    "\n",
    "def is_person_id(rid: str) -> bool:\n",
    "    return rid.endswith('P')\n",
    "\n",
    "result = {}   # company_id → {'invest_person': ..., 'time': ...}\n",
    "\n",
    "for cid in query_ids:\n",
    "    c_time = time_map.get(cid)\n",
    "    node   = id_to_node.get(cid)\n",
    "    if (c_time is None) or (node is None):\n",
    "        result[cid] = {'invest_person': None, 'time': None}\n",
    "        continue\n",
    "\n",
    "    best_investor, best_date = None, -1\n",
    "\n",
    "    for u, v, k, attr in G.edges(node, keys=True, data=True):\n",
    "        edge_date = attr.get('edge_date')\n",
    "        if edge_date is None:\n",
    "            continue\n",
    "        try:\n",
    "            edge_date = int(edge_date)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        if edge_date > c_time:\n",
    "            continue                          \n",
    "\n",
    "        other = v if u == node else u\n",
    "        other_id = G.nodes[other]['id']\n",
    "        if not is_person_id(other_id):\n",
    "            continue                         \n",
    "\n",
    "        if edge_date > best_date:\n",
    "            best_date     = edge_date\n",
    "            best_investor = other_id\n",
    "\n",
    "    result[cid] = {'invest_person': best_investor, 'time': int(c_time)}\n",
    "\n",
    "print(f\"Finished. Got investors for {len(result)} companies.\")\n",
    "\n",
    "with out_json_path.open('w', encoding='utf-8') as f:\n",
    "    json.dump(result, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Graph reasoning path analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "strat_csv   = Path('../sampled_10pct_temporal_stratified.csv')  \n",
    "paths_csv   = Path('../company_two_paths.csv')                   \n",
    "out_json    = Path('../company_two_paths.json')                  \n",
    "\n",
    "df_q = pd.read_csv(strat_csv, dtype={'CompanyID': str})\n",
    "df_q = df_q[(df_q['time'] >= 50) & (df_q['time'] <= 190)]\n",
    "query_ids = set(df_q['CompanyID'])\n",
    "print(f\"Target companies (time in [50,190]): {len(query_ids)}\")\n",
    "\n",
    "df_paths = pd.read_csv(paths_csv, dtype={'CompanyID': str})\n",
    "\n",
    "path_map = dict(\n",
    "    zip(\n",
    "        df_paths['CompanyID'],\n",
    "        zip(df_paths.get('Path1', pd.Series([None]*len(df_paths))),\n",
    "            df_paths.get('Path2', pd.Series([None]*len(df_paths))))\n",
    "    )\n",
    ")\n",
    "\n",
    "PATH_SEP = '|'\n",
    "\n",
    "def _is_nan(x) -> bool:\n",
    "    return x is None or (isinstance(x, float) and math.isnan(x))\n",
    "\n",
    "def parse_path(path_str):\n",
    "    if _is_nan(path_str):\n",
    "        return None\n",
    "    s = str(path_str).strip()\n",
    "    if not s:\n",
    "        return None\n",
    "    return [tok.strip() for tok in s.split(PATH_SEP) if tok.strip()]\n",
    "\n",
    "def check_alternating(path_ids):\n",
    "    if not path_ids or len(path_ids) < 2:\n",
    "        return True\n",
    "    def is_person_id(rid: str) -> bool:\n",
    "        return rid.endswith('P')\n",
    "    last_is_person = is_person_id(path_ids[0])\n",
    "    for rid in path_ids[1:]:\n",
    "        cur_is_person = is_person_id(rid)\n",
    "        if cur_is_person == last_is_person:\n",
    "            return False\n",
    "        last_is_person = cur_is_person\n",
    "    return True\n",
    "\n",
    "\n",
    "result = {}  \n",
    "\n",
    "missing_in_paths = 0\n",
    "bad_alt_count = 0\n",
    "both_ok = 0\n",
    "\n",
    "for cid in query_ids:\n",
    "    p1_str, p2_str = path_map.get(cid, (None, None))\n",
    "    p1 = parse_path(p1_str)\n",
    "    p2 = parse_path(p2_str)\n",
    "\n",
    "    if (p1 is None) and (p2 is None):\n",
    "        missing_in_paths += 1\n",
    "\n",
    "    if p1 and not check_alternating(p1):\n",
    "        bad_alt_count += 1\n",
    "    if p2 and not check_alternating(p2):\n",
    "        bad_alt_count += 1\n",
    "    if p1 and p2:\n",
    "        both_ok += 1\n",
    "\n",
    "    result[cid] = {'path1': p1, 'path2': p2}\n",
    "\n",
    "print(f\"Finished. Companies exported: {len(result)}\")\n",
    "print(f\" - Not found (both paths missing): {missing_in_paths}\")\n",
    "print(f\" - Alternation rule violations (non-fatal): {bad_alt_count}\")\n",
    "print(f\" - Both paths present: {both_ok}\")\n",
    "\n",
    "with out_json.open('w', encoding='utf-8') as f:\n",
    "    json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "print(f\"Wrote JSON to: {out_json.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Construct prompts from four angles and generate corresponding prompt prediction results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we build a prompt for similar companies found by text similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "mapping_json = Path('../qualified_test_to_similar.json')\n",
    "test_pkl     = Path('../test_data_2022_basic.pkl')\n",
    "hist_pkl     = Path('../train_data_2022_basic.pkl')\n",
    "out_pkl      = Path('../text_similar_company_prompt.pkl')\n",
    "\n",
    "with mapping_json.open('r', encoding='utf-8') as f:\n",
    "    mapping = json.load(f)\n",
    "test_ids = list(mapping.keys())\n",
    "\n",
    "with test_pkl.open('rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "with hist_pkl.open('rb') as f:\n",
    "    hist_data = pickle.load(f)\n",
    "\n",
    "all_data = {}\n",
    "all_data.update(hist_data)\n",
    "all_data.update(test_data)\n",
    "\n",
    "intro = (\n",
    "    \"You are a seasoned venture-capital investor. \"\n",
    "    \"A target company has just secured its Series-A financing. \"\n",
    "    \"Your task is to predict whether it will obtain a second round of financing, \"\n",
    "    \"IPO, or be acquired within the next 12 months. \"\n",
    "    \"Below are reference companies (Q) and their outcomes (A). \"\n",
    "    \"In the labels, True means the company succeeded within one year; False means it did not.\\n\"\n",
    ")\n",
    "\n",
    "closing = (\n",
    "    \"### Instructions\"\n",
    "    \"Based on the reference Q-A pairs above and the target company information,\"\n",
    "    \"1. First output your **single-word judgment** on whether the company will obtain a second round, IPO, or be acquired within 12 months.\"\n",
    "    \"   Use exactly one of the following formats:\"\n",
    "    \"      Prediction: True\"\n",
    "    \"      Prediction: False\"\n",
    "    \"\"\n",
    "    \"2. Immediately after that, provide your explanation covering both **positive factors** and **negative factors** that led you to this judgment.\"\n",
    "    \"   You may write as many sentences as you feel necessary.\"\n",
    "    \"\"\n",
    "    \"### Output Format (exactly)\"\n",
    "    \"Prediction: True/False\"\n",
    "    \"Analysis:\"\n",
    "    \"<your detailed analysis here>\"\n",
    ")\n",
    "\n",
    "def bool_str(label: int) -> str:\n",
    "    return \"True\" if label == 1 else \"False\"\n",
    "\n",
    "prompts = {} \n",
    "\n",
    "for cid in test_ids:\n",
    "    ref_ids = mapping.get(cid, [])[:4]\n",
    "    qa_blocks = []\n",
    "\n",
    "    for rid in ref_ids:\n",
    "        ref_obj = all_data.get(rid)\n",
    "        if not ref_obj:\n",
    "            continue\n",
    "        q = ref_obj.get('basic_info', '').strip()\n",
    "        a = bool_str(ref_obj.get('label', 0))\n",
    "        qa_blocks.append(f\"Q: {q}\\nA: {a}\\n\")\n",
    "\n",
    "    target_obj = all_data.get(cid, {})\n",
    "    target_q   = target_obj.get('basic_info', '').strip()\n",
    "    target_section = (\n",
    "        \"Below is the target company's basic information:\\n\"\n",
    "        f\"Q: {target_q}\\n\"\n",
    "    )\n",
    "\n",
    "    prompt = (\n",
    "        f\"{intro}\\n\"\n",
    "        + \"\\n\".join(qa_blocks)\n",
    "        + \"\\n\"\n",
    "        + target_section\n",
    "        + \"\\n\"\n",
    "        + closing\n",
    "    )\n",
    "\n",
    "    prompts[cid] = {\"input_prompt\": prompt}\n",
    "\n",
    "with out_pkl.open('wb') as f:\n",
    "    pickle.dump(prompts, f)\n",
    "\n",
    "print(f\"Generated prompts for {len(prompts)} companies → {out_pkl}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is a prompt for background analysis of the lead investor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "from pathlib import Path\n",
    "import json, pickle, random, sys\n",
    "from collections import defaultdict\n",
    "\n",
    "company_json = Path('../qualified_test_company_to_investor.json')\n",
    "train_pkl    = Path('../train_data_2022_basic.pkl')\n",
    "test_pkl     = Path('../test_data_2022_basic.pkl')\n",
    "graph_pkl    = Path('../filtered_graph_2022_updated_labels.pkl')\n",
    "out_pkl      = Path('../company_prompts_single_investor.pkl')\n",
    "\n",
    "company_map = json.loads(company_json.read_text())\n",
    "\n",
    "with train_pkl.open(\"rb\") as f:\n",
    "    train_data = pickle.load(f)\n",
    "with test_pkl.open(\"rb\") as f:\n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "basic_info_db = {**train_data, **test_data}\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "with graph_pkl.open(\"rb\") as f:\n",
    "    G: nx.MultiDiGraph = pickle.load(f)  \n",
    "\n",
    "id2index = {str(d[\"id\"]): n for n, d in G.nodes(data=True)}\n",
    "\n",
    "def label_str(lbl: int) -> str:\n",
    "    return \"True\" if lbl == 1 else (\"False\" if lbl == 0 else \"Unknown\")\n",
    "\n",
    "\n",
    "def fetch_basic(cid: str):\n",
    "    rec = basic_info_db.get(cid, {})\n",
    "    return rec.get(\"basic_info\", \"\").strip(), label_str(rec.get(\"label\"))\n",
    "\n",
    "\n",
    "def collect_person_history(\n",
    "    person_nd: int, cutoff_time: int, target_cid: str, top_k: int = 5\n",
    "):\n",
    "    invest, position = [], []\n",
    "\n",
    "    for _, nbr, data in G.edges(person_nd, data=True):\n",
    "        edge_time = data.get(\"edge_date\")\n",
    "        if edge_time is None or edge_time >= cutoff_time:\n",
    "            continue \n",
    "\n",
    "        cid_nbr = str(G.nodes[nbr][\"id\"])\n",
    "        if cid_nbr == target_cid:\n",
    "            continue  \n",
    "\n",
    "        edge_type = str(data.get(\"edge_type\")) \n",
    "        if edge_type == \"0\":\n",
    "            invest.append(nbr)\n",
    "        else:\n",
    "            position.append(nbr)\n",
    "\n",
    "    random.shuffle(invest)\n",
    "    random.shuffle(position)\n",
    "    return invest[:top_k], position[:top_k]\n",
    "\n",
    "\n",
    "def build_prompt(person_id: str, target_cid: str, target_time: int, target_info: str):\n",
    "    p_nd = id2index.get(person_id)\n",
    "    if p_nd is None:\n",
    "        raise RuntimeError(\"Investor node not found in graph\")\n",
    "\n",
    "    invest_nodes, pos_nodes = collect_person_history(\n",
    "        p_nd, cutoff_time=target_time, target_cid=target_cid\n",
    "    )\n",
    "\n",
    "    invest_lines = []\n",
    "    for i, n in enumerate(invest_nodes, 1):\n",
    "        cid = str(G.nodes[n][\"id\"])\n",
    "        info, lbl = fetch_basic(cid)\n",
    "        if info:\n",
    "            invest_lines.append(f\"  • Deal {i}: {info} (Label: {lbl})\")\n",
    "\n",
    "    position_lines = []\n",
    "    for i, n in enumerate(pos_nodes, 1):\n",
    "        cid = str(G.nodes[n][\"id\"])\n",
    "        info, lbl = fetch_basic(cid)\n",
    "        if info:\n",
    "            position_lines.append(f\"  • Role {i}: {info} (Label: {lbl})\")\n",
    "\n",
    "    if not invest_lines and not position_lines:\n",
    "        raise RuntimeError(\"All history filtered – empty prompt\")\n",
    "\n",
    "    return \"\\n\".join(\n",
    "        [\n",
    "            \"You are an independent investment analysis expert.\",\n",
    "            f\"The following is Investor {person_id}'s track record *prior to month {target_time}*.\",\n",
    "            \"\",\n",
    "            \"=== Investment History ===\",\n",
    "            *(invest_lines or [\"  (none)\"]),\n",
    "            \"\",\n",
    "            \"=== Board / Executive Positions ===\",\n",
    "            *(position_lines or [\"  (none)\"]),\n",
    "            \"\",\n",
    "            \"=\" * 60,\n",
    "            \"\",\n",
    "            \"Target company that has just closed its Series-A:\",\n",
    "            f\"Q: {target_info}\",\n",
    "            \"\",\n",
    "            \"### Instructions\",\n",
    "            \"• Analyse how the investor's experience relates to the target company.\",\n",
    "            \"• Discuss positive factors, negative factors, and open questions.\",\n",
    "            \"\",\n",
    "            \"Output format (exactly):\",\n",
    "            \"Analysis:\",\n",
    "            \"<your analysis here>\",\n",
    "            \"Prediction: True/False\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "company_prompts = {}\n",
    "fail_cnt = 0\n",
    "\n",
    "for cid, meta in company_map.items():\n",
    "    investor_id = meta.get(\"invest_person\")\n",
    "    tgt_time = int(meta.get(\"time\", -1))\n",
    "    tgt_info, _ = fetch_basic(cid)\n",
    "\n",
    "    if tgt_time < 0 or not investor_id or not tgt_info:\n",
    "        continue  \n",
    "\n",
    "    try:\n",
    "        prompt = build_prompt(investor_id, cid, tgt_time, tgt_info)\n",
    "        company_prompts[cid] = {\"input_prompt\": prompt}\n",
    "    except Exception as e:\n",
    "        fail_cnt += 1\n",
    "        print(f\"[WARN] skip {cid} ({investor_id}): {e}\", file=sys.stderr)\n",
    "\n",
    "print(\n",
    "    f\"\\nGenerated prompts for {len(company_prompts)} companies. \"\n",
    "    f\"Skipped: {fail_cnt}\"\n",
    ")\n",
    "\n",
    "out_pkl.parent.mkdir(parents=True, exist_ok=True)\n",
    "with out_pkl.open(\"wb\") as f:\n",
    "    pickle.dump(company_prompts, f)\n",
    "\n",
    "print(\"Saved →\", out_pkl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is the prompt for building the graph reasoning path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pickle\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "paths_json = Path(\"../company_two_paths.json\")\n",
    "train_pkl  = Path(\"../train_data_2022_basic.pkl\")\n",
    "test_pkl   = Path(\"../test_data_2022_basic.pkl\")\n",
    "graph_pkl  = Path(\"../graph_2022.pkl\")\n",
    "out_pkl    = Path(\"../company_path_prompts_merged.pkl\")\n",
    "\n",
    "company_paths: Dict[str, Dict[str, List[str]]] = json.loads(paths_json.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "with train_pkl.open(\"rb\") as f:\n",
    "    train_data = pickle.load(f)\n",
    "with test_pkl.open(\"rb\") as f:\n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "basic_info_db = {**train_data, **test_data}\n",
    "\n",
    "with graph_pkl.open(\"rb\") as f:\n",
    "    G: nx.MultiDiGraph = pickle.load(f)\n",
    "\n",
    "id2node = {str(d[\"id\"]): n for n, d in G.nodes(data=True)}\n",
    "\n",
    "def is_person_id(rid: str) -> bool:\n",
    "    return isinstance(rid, str) and rid.endswith(\"P\")\n",
    "\n",
    "def label_str(lbl: Optional[int]) -> str:\n",
    "    return \"True\" if lbl == 1 else (\"False\" if lbl == 0 else \"Unknown\")\n",
    "\n",
    "def company_display_name(cid: str) -> str:\n",
    "    rec = basic_info_db.get(cid) or {}\n",
    "    return (rec.get(\"name\") or \"\").strip() or cid\n",
    "\n",
    "def company_profile_and_label(cid: str) -> Tuple[str, str]:\n",
    "    rec = basic_info_db.get(cid) or {}\n",
    "    profile = (rec.get(\"basic_info\") or \"\").strip()\n",
    "    return profile, label_str(rec.get(\"label\"))\n",
    "\n",
    "def person_profile(pid: str) -> str:\n",
    "    nd = id2node.get(pid)\n",
    "    if nd is None:\n",
    "        return f\"Investor {pid}\"\n",
    "    attrs = G.nodes[nd]\n",
    "    name = (attrs.get(\"name\") or \"\").strip()\n",
    "    binfo = (attrs.get(\"basic_info\") or \"\").strip()\n",
    "    if name and binfo:\n",
    "        return f\"{name} — {binfo}\"\n",
    "    if name:\n",
    "        return name\n",
    "    if binfo:\n",
    "        return binfo\n",
    "    return f\"Investor {pid}\"\n",
    "\n",
    "def format_path(ids: Optional[List[str]]) -> str:\n",
    "    if not ids:\n",
    "        return \"(none)\"\n",
    "    return \" -> \".join(ids)\n",
    "\n",
    "def unique_in_order(items: List[str]) -> List[str]:\n",
    "    seen, out = set(), []\n",
    "    for x in items:\n",
    "        if x not in seen:\n",
    "            seen.add(x)\n",
    "            out.append(x)\n",
    "    return out\n",
    "\n",
    "def collect_entities_both_paths(target_cid: str, p1: Optional[List[str]], p2: Optional[List[str]]):\n",
    "    seq = (p1 or []) + (p2 or [])\n",
    "    comp_ids, person_ids = [], []\n",
    "    comp_seen, pers_seen = set(), set()\n",
    "    for rid in seq:\n",
    "        if is_person_id(rid):\n",
    "            if rid not in pers_seen:\n",
    "                pers_seen.add(rid)\n",
    "                person_ids.append(rid)\n",
    "        else:\n",
    "            if rid != target_cid and rid not in comp_seen:\n",
    "                comp_seen.add(rid)\n",
    "                comp_ids.append(rid)\n",
    "    return comp_ids, person_ids\n",
    "\n",
    "def build_merged_prompt(target_cid: str, path1: Optional[List[str]], path2: Optional[List[str]]) -> Optional[str]:\n",
    "    if not (path1 or path2):\n",
    "        return None\n",
    "\n",
    "    target_name = company_display_name(target_cid)\n",
    "    target_profile, _ = company_profile_and_label(target_cid)\n",
    "\n",
    "    path_a = format_path(path1)\n",
    "    path_b = format_path(path2)\n",
    "\n",
    "    comp_ids, person_ids = collect_entities_both_paths(target_cid, path1, path2)\n",
    "\n",
    "    company_profile_lines = []\n",
    "    label_pairs = []\n",
    "    for cid in comp_ids:\n",
    "        info, lbl = company_profile_and_label(cid)\n",
    "        name = company_display_name(cid)\n",
    "        if info:\n",
    "            company_profile_lines.append(f\"  • {name} ({cid}): {info} (Outcome Label: {lbl})\")\n",
    "        else:\n",
    "            company_profile_lines.append(f\"  • {name} ({cid}) (Outcome Label: {lbl})\")\n",
    "        label_pairs.append(f\"{cid}:{lbl}\")\n",
    "    labels_summary = \", \".join(label_pairs) if label_pairs else \"(none)\"\n",
    "\n",
    "    investor_lines = []\n",
    "    for pid in person_ids:\n",
    "        investor_lines.append(f\"  • {person_profile(pid)} ({pid})\")\n",
    "\n",
    "    tgt_block = (target_profile or f\"(No profile text available for {target_name})\").strip()\n",
    "\n",
    "    if path_a == \"(none)\" and path_b == \"(none)\":\n",
    "        return None\n",
    "\n",
    "    lines = [\n",
    "        \"Role: You are a senior venture-capital analyst who excels at step-by-step reasoning over investment paths to judge whether a seed/angel-stage start-up is likely to secure Series-A funding within the next year.\",\n",
    "        \"\",\n",
    "        \"You are given the following information blocks:\",\n",
    "        f\"(1) High-value investment paths retrieved for {target_name} ({target_cid}):\",\n",
    "        f\"    (A) {path_a}\",\n",
    "        f\"    (B) {path_b}\",\n",
    "        \"(2) Company profiles appearing in the paths (each with outcome labels; True = raised Series A within 12 months after seed/angel, False = did not):\",\n",
    "        *(company_profile_lines or [\"  (none)\"]),\n",
    "        f\"    Success/Failure: {labels_summary}\",\n",
    "        \"(3) Investor profiles appearing in the paths:\",\n",
    "        *(investor_lines or [\"  (none)\"]),\n",
    "        \"(4) Target company profile:\",\n",
    "        f\"    {tgt_block}\",\n",
    "        \"\",\n",
    "        \"Task:\",\n",
    "        f\"• Analyse the evidence and predict whether {target_name} will raise a Series-A round within 12 months.\",\n",
    "        \"\",\n",
    "        \"Output exactly in the format:\",\n",
    "        \"Prediction: True/False\",\n",
    "        \"Analysis: <your step-by-step reasoning>\",\n",
    "        \"\",\n",
    "        \"If evidence is insufficient, reason cautiously but still decide.\",\n",
    "    ]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "out: Dict[str, Dict[str, object]] = {}\n",
    "skipped = 0\n",
    "\n",
    "for cid, obj in company_paths.items():\n",
    "    p1 = obj.get(\"path1\") or None\n",
    "    p2 = obj.get(\"path2\") or None\n",
    "\n",
    "    prompt = build_merged_prompt(cid, p1, p2)\n",
    "    if not prompt:\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    out[cid] = {\n",
    "        \"paths\": {\"path1\": p1, \"path2\": p2},\n",
    "        \"input_prompt\": prompt,\n",
    "    }\n",
    "\n",
    "out_pkl.parent.mkdir(parents=True, exist_ok=True)\n",
    "with out_pkl.open(\"wb\") as f:\n",
    "    pickle.dump(out, f)\n",
    "\n",
    "print(\"Saved →\", out_pkl.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. After getting the required prompt, the following is to call LLM on these prompts in parallel to get the return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import os, pickle, json\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "IN_PKLS = [\n",
    "    Path('../text_similar_company_prompt.pkl'),\n",
    "    Path('../company_path_prompts_merged.pkl'),\n",
    "    Path('../company_prompts_single_investor.pkl'),\n",
    "]\n",
    "\n",
    "def out_path(in_path: Path) -> Path:\n",
    "    return in_path.with_name(in_path.stem + \"_with_pred.pkl\")\n",
    "\n",
    "BASE_URL = os.getenv(\"BASE_URL\", \"\")\n",
    "MODEL    = \"\"\n",
    "\n",
    "api_keys = [\n",
    "    os.getenv(\"OPENAI_API_KEY\"),\n",
    "    os.getenv(\"OPENAI_API_KEY_2\"),\n",
    "    os.getenv(\"OPENAI_API_KEY_3\"),\n",
    "    os.getenv(\"OPENAI_API_KEY_4\"),\n",
    "    os.getenv(\"OPENAI_API_KEY_5\"),\n",
    "    os.getenv(\"OPENAI_API_KEY_6\"),\n",
    "    os.getenv(\"OPENAI_API_KEY_7\"),\n",
    "    os.getenv(\"OPENAI_API_KEY_8\"),\n",
    "]\n",
    "api_keys = [k for k in api_keys if k]\n",
    "\n",
    "clients = [OpenAI(api_key=k, base_url=BASE_URL) for k in api_keys for _ in range(2)]\n",
    "MAX_WORKERS = len(clients)\n",
    "\n",
    "def call_llm(prompt: str, client: OpenAI) -> str:\n",
    "    resp = client.chat.completions.create(\n",
    "        model       = MODEL,\n",
    "        messages    = [{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature = 0.0,\n",
    "        \n",
    "    )\n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "SAVE_EVERY_N = 50      \n",
    "\n",
    "def process_pkl(in_pkl: Path):\n",
    "    out_pkl = out_path(in_pkl)\n",
    "    with in_pkl.open('rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    if out_pkl.exists():\n",
    "        with out_pkl.open('rb') as f:\n",
    "            saved = pickle.load(f)\n",
    "        for k, v in saved.items():\n",
    "            if isinstance(v, dict) and v.get('prediction'):\n",
    "                data.setdefault(k, {}).update({'prediction': v['prediction']})\n",
    "\n",
    "    tasks = []\n",
    "    for i, (cid, rec) in enumerate(data.items()):\n",
    "        if rec.get('prediction'):     \n",
    "            continue\n",
    "        prompt = rec.get('input_prompt', '').strip()\n",
    "        if not prompt:\n",
    "            data[cid]['prediction'] = \"\"\n",
    "            continue\n",
    "        client = clients[i % MAX_WORKERS]\n",
    "        tasks.append((cid, prompt, client))\n",
    "\n",
    "    if not tasks:\n",
    "        print(f\"[Skip] {in_pkl.name} \")\n",
    "        return\n",
    "\n",
    "    pending = tasks[:]\n",
    "    completed = 0\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as exe:\n",
    "        futures = {exe.submit(call_llm, p, c): cid for cid, p, c in pending}\n",
    "        pbar = tqdm(total=len(futures), desc=f\"LLM predicting ({in_pkl.name})\")\n",
    "        for fut in as_completed(futures):\n",
    "            cid = futures[fut]\n",
    "            try:\n",
    "                data[cid]['prediction'] = fut.result()\n",
    "            except Exception as e:\n",
    "                print(f\"[Error] {cid}: {e}\")\n",
    "                data[cid]['prediction'] = \"\"\n",
    "            completed += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "            if completed % SAVE_EVERY_N == 0:\n",
    "                with out_pkl.open('wb') as f:\n",
    "                    pickle.dump(data, f)\n",
    "        pbar.close()\n",
    "\n",
    "    with out_pkl.open('wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"[Done] {in_pkl.name} → {out_pkl}\")\n",
    "\n",
    "for pkl_path in IN_PKLS:\n",
    "    if not pkl_path.exists():\n",
    "        print(f\"[WARN] {pkl_path}\")\n",
    "        continue\n",
    "    process_pkl(pkl_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Next, we use the text encoder to preprocess all LLM output content to facilitate subsequent training of the weight generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import pickle, numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "IN_PKL   = Path('../combined_predictions.pkl')\n",
    "EMB_NPZ  = Path('../text_embed.npz')\n",
    "LBL_NPY  = Path('../labels.npy')\n",
    "\n",
    "VIEW_KEYS = ['c', 'p', 'pp']      \n",
    "MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "with IN_PKL.open('rb') as f:\n",
    "    raw = pickle.load(f)     \n",
    "\n",
    "company_ids = sorted(raw.keys())\n",
    "N = len(company_ids)\n",
    "print(f\"Total samples: {N}\")\n",
    "\n",
    "texts = []\n",
    "labels = np.zeros((N, 3), dtype=np.int8)\n",
    "\n",
    "for i, cid in enumerate(company_ids):\n",
    "    for j, vk in enumerate(VIEW_KEYS):\n",
    "        view = raw[cid][vk]\n",
    "        texts.append(view['text'])\n",
    "        labels[i, j] = int(view['label'])\n",
    "\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "emb = model.encode(\n",
    "    texts,\n",
    "    batch_size=64,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True,\n",
    "    show_progress_bar=True\n",
    ")  \n",
    "\n",
    "vectors_text = emb.reshape(N, 4, -1).astype('float32')\n",
    "\n",
    "print(\"vectors_text shape :\", vectors_text.shape)  \n",
    "print(\"labels shape       :\", labels.shape)       \n",
    "\n",
    "np.savez_compressed(\n",
    "    EMB_NPZ,\n",
    "    company_ids=np.array(company_ids, dtype='<U20'),\n",
    "    view_keys=np.array(VIEW_KEYS),\n",
    "    vectors_text=vectors_text\n",
    ")\n",
    "\n",
    "np.save(LBL_NPY, labels)\n",
    "\n",
    "print(f\"Saved embeddings → {EMB_NPZ}\")\n",
    "print(f\"Saved labels     → {LBL_NPY}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. This is the part that quantifies the basic features of the company, which is used by the weight generator to generate different weights based on the company's features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv1_path = '../Company.csv'\n",
    "csv2_path = '../CompanyIndustryRelation.csv'\n",
    "csv3_path = '../CompanyFinancialRelation.csv'\n",
    "csv4_path = '../CompanyInvestorRelation.csv'\n",
    "csv5_path = '../CompanyEmployeeHistoryRelation.csv'\n",
    "\n",
    "df1 = pd.read_csv(csv1_path, nrows=0)\n",
    "df2 = pd.read_csv(csv2_path, nrows=0)\n",
    "df3 = pd.read_csv(csv3_path, nrows=0)\n",
    "df4 = pd.read_csv(csv4_path, nrows=0)\n",
    "df5 = pd.read_csv(csv5_path, nrows=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd, numpy as np, json, torch\n",
    "from pathlib import Path\n",
    "\n",
    "ID2JSON = Path('../ID2index.json')\n",
    "COMP_CSV = Path('../qualified_new_company.csv')\n",
    "COMP_RAW = Path('../Company.csv')\n",
    "DEAL_RAW = Path('../Deal.csv')\n",
    "\n",
    "ID2index = json.loads(ID2JSON.read_text())\n",
    "target_ids = pd.read_csv(COMP_CSV, dtype=str)['CompanyID'].tolist()\n",
    "\n",
    "target_ids = [cid for cid in target_ids if cid in ID2index]\n",
    "print(f\"Target companies in mapping: {len(target_ids)}\")\n",
    "\n",
    "company_df = pd.read_csv(COMP_RAW)\n",
    "deal_df    = pd.read_csv(DEAL_RAW)\n",
    "\n",
    "company_df = company_df[ company_df['CompanyID'].isin(target_ids) ]\n",
    "deal_df    = deal_df[ deal_df['CompanyID'].isin(target_ids) & (deal_df['DealNo']==1) ]\n",
    "\n",
    "company_df['Index'] = company_df['CompanyID'].map(ID2index)\n",
    "deal_df['Index']    = deal_df['CompanyID'].map(ID2index)\n",
    "\n",
    "company_df = company_df.set_index('Index', drop=False)\n",
    "deal_df    = deal_df.set_index('Index', drop=False)\n",
    "\n",
    "industry_list   = sorted(company_df['PrimaryIndustryGroup'].dropna().unique())\n",
    "dealtype_list   = sorted(deal_df['DealType'].dropna().unique())\n",
    "\n",
    "Industry2idx = {name:i for i,name in enumerate(industry_list)}\n",
    "DealType2idx = {name:i for i,name in enumerate(dealtype_list)}\n",
    "\n",
    "print(\"Industry categories :\", len(Industry2idx))\n",
    "print(\"DealType categories :\", len(DealType2idx))\n",
    "\n",
    "dim = 2 + len(Industry2idx) + len(DealType2idx)\n",
    "mat = torch.zeros(len(ID2index), dim, dtype=torch.float32)   \n",
    "\n",
    "for idx,row in company_df.iterrows():\n",
    "    mat[idx, 0] = row['YearFounded'] if not pd.isna(row['YearFounded']) else 0\n",
    "    industry = row['PrimaryIndustryGroup']\n",
    "    if industry in Industry2idx:\n",
    "        mat[idx, 2 + Industry2idx[industry]] = 1\n",
    "\n",
    "base_dt = 2 + len(Industry2idx)\n",
    "for idx,row in deal_df.iterrows():\n",
    "    mat[idx, 1] = row['DealSize'] if not pd.isna(row['DealSize']) else 0\n",
    "    dt = row['DealType']\n",
    "    if dt in DealType2idx:\n",
    "        mat[idx, base_dt + DealType2idx[dt]] = 1\n",
    "\n",
    "row_idx = [ID2index[cid] for cid in target_ids]         \n",
    "attr_matrix = mat[row_idx].numpy()                      \n",
    "\n",
    "np.save('company_attr_target.npy', attr_matrix)\n",
    "np.save('company_id_order.npy', np.array(target_ids))\n",
    "\n",
    "print(\"Saved attr matrix :\", attr_matrix.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VC_LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
