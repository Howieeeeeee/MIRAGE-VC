{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Select some points for training, these points are very rich in connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os, math, json, pickle, random\n",
    "from collections import deque\n",
    "from typing import Dict, Any, List\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "GRAPH_PKL  = \"../graph_2022.pkl\"\n",
    "OUT_DIR    = \"../path_selector/graph_retrieval_data\"\n",
    "\n",
    "TRAIN_SIZE = 1400\n",
    "VALID_SIZE = 400\n",
    "TEST_SIZE  = 200\n",
    "\n",
    "RANDOM_SEED              = 42\n",
    "FAST_SAMPLE_ENABLED      = True  \n",
    "FAST_SAMPLE_OVERSAMPLE   = 1.35   \n",
    "FAST_MIN_DEG1            = 1     \n",
    "FAST_ACCEPT_RATIO        = 0.7   \n",
    "FAST_PERCENTILE          = 0.50   \n",
    "FAST_BATCH_SIZE          = 500    \n",
    "MAX_FAST_ITER_MULTIPLIER = 8      \n",
    "\n",
    "MAX_HOPS    = 4\n",
    "LAMBDA_TIME = 0.01\n",
    "T_REF       = 192\n",
    "\n",
    "W_DEG1    = 0.4\n",
    "W_EDGES   = 0.3\n",
    "W_NODES   = 0.2\n",
    "W_ENTROPY = 0.1\n",
    "W_DECAY   = 0.05\n",
    "\n",
    "F_W_DEG1       = 0.5\n",
    "F_W_COMP_PROP  = 0.2\n",
    "F_W_TWOHOP     = 0.3\n",
    "\n",
    "ENABLE_DIVERSITY_POST   = False\n",
    "INV_OVERLAP_THRESHOLD   = 0.7\n",
    "\n",
    "MAX_SUBGRAPH_NODES = 5000   \n",
    "\n",
    "def is_company(g, node_key) -> bool:\n",
    "    real_id = g.nodes[node_key].get('id')\n",
    "    if real_id is None:\n",
    "        raise ValueError(f\"Node {node_key} missing 'id' attribute\")\n",
    "    return not str(real_id).endswith('P')\n",
    "\n",
    "def k_hop_subgraph_nodes(g: nx.Graph, root, max_hops: int):\n",
    "    visited = {root}\n",
    "    frontier = deque([(root, 0)])\n",
    "    while frontier:\n",
    "        u, d = frontier.popleft()\n",
    "        if d == max_hops:\n",
    "            continue\n",
    "        for v in g.neighbors(u):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                if len(visited) >= MAX_SUBGRAPH_NODES:\n",
    "                    return visited\n",
    "                frontier.append((v, d+1))\n",
    "    return visited\n",
    "\n",
    "def type_entropy(comp_cnt: int, inv_cnt: int) -> float:\n",
    "    total = comp_cnt + inv_cnt\n",
    "    if total == 0: return 0.0\n",
    "    out = 0.0\n",
    "    for c in (comp_cnt, inv_cnt):\n",
    "        if c > 0:\n",
    "            p = c / total\n",
    "            out -= p * math.log(p + 1e-12)\n",
    "    return out\n",
    "\n",
    "def compute_metrics(g: nx.Graph, node_id, is_company_flag) -> Dict[str, Any]:\n",
    "    sub_nodes = k_hop_subgraph_nodes(g, node_id, MAX_HOPS)\n",
    "    edge_cnt = 0\n",
    "    for v in sub_nodes:\n",
    "        for u in g.neighbors(v):\n",
    "            if u in sub_nodes and u > v:\n",
    "                edge_cnt += 1\n",
    "    comp_cnt = sum(is_company_flag[n] for n in sub_nodes)\n",
    "    inv_cnt  = len(sub_nodes) - comp_cnt\n",
    "    ent = type_entropy(comp_cnt, inv_cnt)\n",
    "    deg1 = len(neighbors[node_id])\n",
    "\n",
    "    decay_sum = 0.0\n",
    "    edge_dates = []\n",
    "    for v in sub_nodes:\n",
    "        for u in g.neighbors(v):\n",
    "            if u in sub_nodes and u > v:\n",
    "                data = g.get_edge_data(v, u)\n",
    "                ed = data.get(\"edge_date\") if isinstance(data, dict) else None\n",
    "                if ed is not None:\n",
    "                    edge_dates.append(ed)\n",
    "                    decay_sum += math.exp(-LAMBDA_TIME * max(T_REF - ed, 0))\n",
    "    avg_edge_date = float(np.mean(edge_dates)) if edge_dates else -1.0\n",
    "\n",
    "    richness = (\n",
    "        math.log(1 + deg1)          * W_DEG1 +\n",
    "        math.log(1 + edge_cnt)      * W_EDGES +\n",
    "        math.log(1 + len(sub_nodes))* W_NODES +\n",
    "        ent * W_ENTROPY +\n",
    "        decay_sum * W_DECAY\n",
    "    )\n",
    "\n",
    "    return dict(\n",
    "        node_key=node_id,\n",
    "        deg1=deg1,\n",
    "        hop_nodes=len(sub_nodes),\n",
    "        hop_edges=edge_cnt,\n",
    "        investor_ratio=inv_cnt / max(1, len(sub_nodes)),\n",
    "        type_entropy=ent,\n",
    "        avg_edge_date=avg_edge_date,\n",
    "        time_decay_sum=decay_sum,\n",
    "        richness_score=richness\n",
    "    )\n",
    "\n",
    "def diversity_postprocess(g: nx.Graph, metrics: List[Dict[str, Any]], need_total: int):\n",
    "    selected = []\n",
    "    seen_investors = set()\n",
    "    for m in metrics:\n",
    "        inv_neigh = {nbr for nbr in g.neighbors(m[\"id\"]) if str(g.nodes[nbr].get('id','')).endswith('P')}\n",
    "        if len(inv_neigh) == 0:\n",
    "            continue\n",
    "        overlap_ratio = len(seen_investors & inv_neigh) / len(inv_neigh)\n",
    "        if overlap_ratio < INV_OVERLAP_THRESHOLD:\n",
    "            selected.append(m)\n",
    "            seen_investors |= inv_neigh\n",
    "        if len(selected) >= need_total:\n",
    "            break\n",
    "    return selected\n",
    "\n",
    "neighbors = {}\n",
    "is_company_flag = {}\n",
    "two_hop_size = {}\n",
    "\n",
    "def precompute_fast_features(g, company_nodes):\n",
    "    global neighbors, is_company_flag, two_hop_size\n",
    "    for n in g.nodes():\n",
    "        neighbors[n] = list(g.neighbors(n))\n",
    "        rid = str(g.nodes[n].get('id',''))\n",
    "        is_company_flag[n] = (not rid.endswith('P'))\n",
    "    for n in company_nodes:\n",
    "        lvl1 = neighbors[n]\n",
    "        seen = set(lvl1)\n",
    "        for v in lvl1:\n",
    "            for u in neighbors[v]:\n",
    "                if u != n:\n",
    "                    seen.add(u)\n",
    "        two_hop_size[n] = len(seen)\n",
    "\n",
    "def fast_score(n):\n",
    "    deg1 = len(neighbors[n])\n",
    "    if deg1 == 0:\n",
    "        return 0.0\n",
    "    comp1 = sum(is_company_flag[u] for u in neighbors[n])\n",
    "    comp_prop = comp1 / deg1\n",
    "    th = two_hop_size.get(n, deg1)  # fallback\n",
    "    return (deg1 * F_W_DEG1 +\n",
    "            th   * F_W_TWOHOP +\n",
    "            comp_prop * F_W_COMP_PROP)\n",
    "\n",
    "def stageA_random_collect(g, company_nodes, need_total):\n",
    "    target = int(need_total * FAST_SAMPLE_OVERSAMPLE)\n",
    "    print(f\"[INFO] StageA target (oversampled) = {target}\")\n",
    "    shuffled = company_nodes[:]   \n",
    "    random.shuffle(shuffled)\n",
    "\n",
    "    collected = []\n",
    "    fast_scores_batch = []\n",
    "    threshold = 0.0\n",
    "\n",
    "    max_iters = min(len(shuffled), need_total * MAX_FAST_ITER_MULTIPLIER)\n",
    "    idx = 0\n",
    "    while len(collected) < target and idx < max_iters:\n",
    "        n = shuffled[idx % len(shuffled)]\n",
    "        idx += 1\n",
    "        if len(neighbors[n]) < FAST_MIN_DEG1:\n",
    "            continue\n",
    "        s = fast_score(n)\n",
    "        fast_scores_batch.append(s)\n",
    "\n",
    "        if len(fast_scores_batch) >= FAST_BATCH_SIZE:\n",
    "            threshold = np.percentile(fast_scores_batch, FAST_PERCENTILE * 100)\n",
    "            fast_scores_batch.clear()\n",
    "\n",
    "        accept = (threshold == 0.0) or (s >= threshold * FAST_ACCEPT_RATIO)\n",
    "        if accept:\n",
    "            collected.append(n)\n",
    "\n",
    "    print(f\"[INFO] StageA collected={len(collected)} after {idx} iterations (thresholdâ‰ˆ{threshold:.3f})\")\n",
    "    if len(collected) < need_total:\n",
    "        print(\"[WARN] StageA collected fewer than need_total; will proceed.\")\n",
    "    return collected[:target]\n",
    "\n",
    "def main():\n",
    "    random.seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "    print(f\"[INFO] Loading graph: {GRAPH_PKL}\")\n",
    "    with open(GRAPH_PKL, \"rb\") as f:\n",
    "        g = pickle.load(f)\n",
    "    print(f\"[INFO] Loaded graph |V|={g.number_of_nodes()} |E|={g.number_of_edges()}\")\n",
    "\n",
    "    company_nodes = [n for n in g.nodes() if is_company(g, n)]\n",
    "    print(f\"[INFO] Company nodes total: {len(company_nodes)}\")\n",
    "    need_total = TRAIN_SIZE + VALID_SIZE + TEST_SIZE\n",
    "\n",
    "    print(\"[INFO] Precomputing fast features...\")\n",
    "    precompute_fast_features(g, company_nodes)\n",
    "    print(\"[INFO] Fast features done.\")\n",
    "\n",
    "    if FAST_SAMPLE_ENABLED:\n",
    "        stageA = stageA_random_collect(g, company_nodes, need_total)\n",
    "    else:\n",
    "        stageA = company_nodes[:]\n",
    "        random.shuffle(stageA)\n",
    "        stageA = stageA[:need_total]\n",
    "\n",
    "    print(f\"[INFO] StageB precise richness on {len(stageA)} nodes (this is main cost).\")\n",
    "    metrics = []\n",
    "    for n in tqdm(stageA, desc=\"StageB compute richness\"):\n",
    "        metrics.append(compute_metrics(g, n, is_company_flag))\n",
    "\n",
    "    metrics.sort(key=lambda x: x[\"richness_score\"], reverse=True)\n",
    "\n",
    "    if ENABLE_DIVERSITY_POST:\n",
    "        filtered = diversity_postprocess(g, metrics, need_total)\n",
    "        if len(filtered) >= need_total:\n",
    "            metrics = filtered + metrics[len(filtered):]\n",
    "        else:\n",
    "            print(\"[WARN] diversity_postprocess insufficient; fallback to original ranking.\")\n",
    "\n",
    "    top_metrics = metrics[:need_total]\n",
    "\n",
    "    def real_id(n):\n",
    "        rid = g.nodes[n].get('id')\n",
    "        if rid is None:\n",
    "            raise ValueError(f\"Node {n} missing 'id' attribute when exporting.\")\n",
    "        return str(rid)\n",
    "\n",
    "    train_keys = [m[\"node_key\"] for m in top_metrics[:TRAIN_SIZE]]\n",
    "    valid_keys = [m[\"node_key\"] for m in top_metrics[TRAIN_SIZE:TRAIN_SIZE+VALID_SIZE]]\n",
    "    test_keys  = [m[\"node_key\"] for m in top_metrics[TRAIN_SIZE+VALID_SIZE:TRAIN_SIZE+VALID_SIZE+TEST_SIZE]]\n",
    "\n",
    "    train_ids = [real_id(k) for k in train_keys]\n",
    "    valid_ids = [real_id(k) for k in valid_keys]\n",
    "    test_ids  = [real_id(k) for k in test_keys]\n",
    "\n",
    "    with open(os.path.join(OUT_DIR, \"train_companies.txt\"), \"w\") as f:\n",
    "        f.write(\"\\n\".join(train_ids))\n",
    "    with open(os.path.join(OUT_DIR, \"valid_companies.txt\"), \"w\") as f:\n",
    "        f.write(\"\\n\".join(valid_ids))\n",
    "    if TEST_SIZE > 0:\n",
    "        with open(os.path.join(OUT_DIR, \"test_companies.txt\"), \"w\") as f:\n",
    "            f.write(\"\\n\".join(test_ids))\n",
    "\n",
    "    with open(os.path.join(OUT_DIR, \"richness_stats.jsonl\"), \"w\") as f:\n",
    "        for m in metrics:\n",
    "            f.write(json.dumps(m, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"[DONE] Train={len(train_ids)} Valid={len(valid_ids)} Test={len(test_ids)}\")\n",
    "    print(\"[HINT] Top-3 richness examples:\")\n",
    "    for m in top_metrics[:3]:\n",
    "        print(m)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Construct basic text information for the selected points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "export_path_groups_3hop.py\n",
    "--------------------------\n",
    "Starting from target companies in the train/valid/test splits, perform \"random-branch 3-hop\" sampling.\n",
    "For each parent path, generate a set (â‰¤3) of candidate extensions. The output is training data for\n",
    "subsequent \"in-group ranking & selection\" (an abstract representation of three binary pairs per group).\n",
    "\n",
    "Each group structure:\n",
    "{\n",
    "  \"target_real_id\": <root company>,\n",
    "  \"hop\": <1|2|3>,                        # depth of this extension (length of the parent path)\n",
    "  \"parent_path\": [\"A\", \"B\", ...],        # sequence of real IDs along the parent path\n",
    "  \"parent_label\": \"A|B|...\",             # compressed parent path label (joined by '|')\n",
    "  \"candidates\": [\"E\",\"F\",\"G\"],           # â‰¤3 new candidate real IDs\n",
    "  \"group_list\": [\"A|B\", \"E\", \"F\", \"G\"]   # list-form representation consistent with your example\n",
    "}\n",
    "\n",
    "Output files:\n",
    "  - path_groups_3hop.jsonl : one JSON group per line\n",
    "  - missing_ids.txt        : root IDs from the splits not present in the graph (if any)\n",
    "  - stats.json             : brief statistics (number of groups, per-hop distribution, etc.)\n",
    "\n",
    "Paths & parameters are fixed; run directly: python export_path_groups_3hop.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "from typing import List, Dict, Any, Set\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "\n",
    "GRAPH_PKL   = \"../graph_2022.pkl\"\n",
    "SPLITS_DIR  = \"../graph_retrieval_data\"\n",
    "OUT_DIR     = \"../graph_retrieval_data/context\"\n",
    "\n",
    "TRAIN_FILE  = \"train_companies.txt\"\n",
    "VALID_FILE  = \"valid_companies.txt\"\n",
    "TEST_FILE   = \"test_companies.txt\"      \n",
    "\n",
    "MAX_HOPS    = 3         \n",
    "SAMPLE_K    = 3          \n",
    "RANDOM_SEED = 42\n",
    "PATH_JOIN   = \"|\"        \n",
    "\n",
    "SHOW_TOP_EXAMPLES = 5    \n",
    "ALLOW_DUP_IN_SIBLINGS = False  \n",
    "\n",
    "def load_graph(path: str):\n",
    "    with open(path, \"rb\") as f:\n",
    "        g = pickle.load(f)\n",
    "    if not isinstance(g, (nx.Graph, nx.DiGraph, nx.MultiGraph, nx.MultiDiGraph)):\n",
    "        raise TypeError(f\"Loaded object type {type(g)} is not a NetworkX graph.\")\n",
    "    return g\n",
    "\n",
    "def load_id_list(path: str) -> List[str]:\n",
    "    if not os.path.exists(path):\n",
    "        return []\n",
    "    with open(path, \"r\") as f:\n",
    "        return [ln.strip() for ln in f if ln.strip()]\n",
    "\n",
    "def build_realid_index(g: nx.Graph) -> Dict[str, List[Any]]:\n",
    "    idx = {}\n",
    "    for nk in g.nodes():\n",
    "        rid = g.nodes[nk].get(\"id\")\n",
    "        if rid is None:\n",
    "            continue\n",
    "        rid = str(rid)\n",
    "        idx.setdefault(rid, []).append(nk)\n",
    "    dups = sum(1 for v in idx.values() if len(v) > 1)\n",
    "    if dups > 0:\n",
    "        print(f\"[WARN] {dups} real ids map to multiple internal node keys (use the first).\")\n",
    "    return idx\n",
    "\n",
    "def sample_children(g: nx.Graph, node_key, visited_keys: Set[Any], k: int) -> List[Any]:\n",
    "    neighs = [nbr for nbr in g.neighbors(node_key) if nbr not in visited_keys]\n",
    "    if not neighs:\n",
    "        return []\n",
    "    random.shuffle(neighs)\n",
    "    picked = neighs[:k]\n",
    "    if (not ALLOW_DUP_IN_SIBLINGS) and len(set(picked)) != len(picked):\n",
    "        picked = list(dict.fromkeys(picked))\n",
    "    return picked\n",
    "\n",
    "def path_label(path_nodes: List[str]) -> str:\n",
    "    return PATH_JOIN.join(path_nodes)\n",
    "\n",
    "def extract_real_id(g: nx.Graph, node_key) -> str:\n",
    "    rid = g.nodes[node_key].get(\"id\")\n",
    "    return str(rid) if rid is not None else None\n",
    "\n",
    "def generate_groups_for_root(g: nx.Graph, root_real_id: str, realid_index: Dict[str, List[Any]]) -> List[Dict[str, Any]]:\n",
    "    groups = []\n",
    "    root_key = realid_index[root_real_id][0]\n",
    "    visited_keys: Set[Any] = {root_key}\n",
    "\n",
    "    parents = [{\n",
    "        \"keys\": [root_key],\n",
    "        \"real_ids\": [root_real_id],\n",
    "        \"last_key\": root_key\n",
    "    }]\n",
    "\n",
    "    for hop in range(1, MAX_HOPS + 1):\n",
    "        next_parents = []\n",
    "        for parent in parents:\n",
    "            parent_last_key = parent[\"last_key\"]\n",
    "            children_keys = sample_children(g, parent_last_key, visited_keys, SAMPLE_K)\n",
    "            if not children_keys:\n",
    "                continue\n",
    "\n",
    "            parent_path_real = parent[\"real_ids\"]  # list[str]\n",
    "            parent_lbl = path_label(parent_path_real)\n",
    "            child_real_ids = []\n",
    "            for ck in children_keys:\n",
    "                rid = extract_real_id(g, ck)\n",
    "                if rid is None:\n",
    "                    continue\n",
    "                child_real_ids.append(rid)\n",
    "\n",
    "            if not child_real_ids:\n",
    "                continue\n",
    "\n",
    "            group_list = [parent_lbl] + child_real_ids\n",
    "            groups.append({\n",
    "                \"target_real_id\": root_real_id,\n",
    "                \"hop\": hop,\n",
    "                \"parent_path\": parent_path_real,\n",
    "                \"parent_label\": parent_lbl,\n",
    "                \"candidates\": child_real_ids,\n",
    "                \"group_list\": group_list\n",
    "            })\n",
    "\n",
    "            for ck, crid in zip(children_keys, child_real_ids):\n",
    "                visited_keys.add(ck)\n",
    "                next_parents.append({\n",
    "                    \"keys\": parent[\"keys\"] + [ck],\n",
    "                    \"real_ids\": parent_path_real + [crid],\n",
    "                    \"last_key\": ck\n",
    "                })\n",
    "\n",
    "        parents = next_parents\n",
    "        if not parents:\n",
    "            break\n",
    "\n",
    "    return groups\n",
    "\n",
    "def main():\n",
    "    random.seed(RANDOM_SEED)\n",
    "\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "    print(f\"[INFO] Loading graph: {GRAPH_PKL}\")\n",
    "    g = load_graph(GRAPH_PKL)\n",
    "    print(f\"[INFO] Graph loaded |V|={g.number_of_nodes()} |E|={g.number_of_edges()}\")\n",
    "\n",
    "    train_ids = load_id_list(os.path.join(SPLITS_DIR, TRAIN_FILE))\n",
    "    valid_ids = load_id_list(os.path.join(SPLITS_DIR, VALID_FILE))\n",
    "    test_ids  = load_id_list(os.path.join(SPLITS_DIR, TEST_FILE))\n",
    "    target_ids = list(dict.fromkeys(train_ids + valid_ids + test_ids))\n",
    "\n",
    "    print(f\"[INFO] Split counts: train={len(train_ids)} valid={len(valid_ids)} test={len(test_ids)} unique_total={len(target_ids)}\")\n",
    "\n",
    "    realid_index = build_realid_index(g)\n",
    "\n",
    "    missing = [rid for rid in target_ids if rid not in realid_index]\n",
    "    if missing:\n",
    "        miss_path = os.path.join(OUT_DIR, \"missing_ids.txt\")\n",
    "        with open(miss_path, \"w\") as f:\n",
    "            f.write(\"\\n\".join(missing))\n",
    "        print(f\"[WARN] {len(missing)} root ids not found. Wrote {miss_path}\")\n",
    "    target_ids = [rid for rid in target_ids if rid in realid_index]\n",
    "\n",
    "    out_groups_path = os.path.join(OUT_DIR, \"path_groups_3hop.jsonl\")\n",
    "    hop_counts = {1: 0, 2: 0, 3: 0}\n",
    "    total_groups = 0\n",
    "\n",
    "    print(f\"[INFO] Generating 3-hop path groups (SAMPLE_K={SAMPLE_K}) ...\")\n",
    "    with open(out_groups_path, \"w\") as fout:\n",
    "        for rid in tqdm(target_ids, desc=\"Targets\"):\n",
    "            root_groups = generate_groups_for_root(g, rid, realid_index)\n",
    "            for gp in root_groups:\n",
    "                hop_counts[gp[\"hop\"]] += 1\n",
    "                total_groups += 1\n",
    "                fout.write(json.dumps(gp, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    stats = {\n",
    "        \"total_targets\": len(target_ids),\n",
    "        \"total_groups\": total_groups,\n",
    "        \"groups_per_hop\": hop_counts,\n",
    "        \"max_hops\": MAX_HOPS,\n",
    "        \"sample_k\": SAMPLE_K,\n",
    "        \"random_seed\": RANDOM_SEED\n",
    "    }\n",
    "    with open(os.path.join(OUT_DIR, \"stats.json\"), \"w\") as f:\n",
    "        json.dump(stats, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"[INFO] Wrote groups to: {out_groups_path}\")\n",
    "    print(f\"[INFO] Stats: {stats}\")\n",
    "\n",
    "    if SHOW_TOP_EXAMPLES > 0:\n",
    "        print(\"[HINT] Example groups:\")\n",
    "        with open(out_groups_path, \"r\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= SHOW_TOP_EXAMPLES:\n",
    "                    break\n",
    "                print(line.strip())\n",
    "\n",
    "    print(\"[DONE] export_path_groups_3hop complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The following is stratified sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "CONTEXT_DIR   = \"../graph_retrieval_data/context\"\n",
    "INPUT_FILE    = \"path_groups_3hop.jsonl\"           \n",
    "OUTPUT_FILE   = \"path_groups_3hop_sampled.jsonl\"   \n",
    "STATS_FILE    = \"path_groups_3hop_sampled_stats.json\"\n",
    "\n",
    "RANDOM_SEED   = 42\n",
    "\n",
    "SAMPLE_RATIO = {\n",
    "    1: 1.0,    \n",
    "    2: 0.5,    \n",
    "    3: 1/3      \n",
    "}\n",
    "\n",
    "USE_FLOOR = True\n",
    "\n",
    "def main():\n",
    "    random.seed(RANDOM_SEED)\n",
    "    in_path  = os.path.join(CONTEXT_DIR, INPUT_FILE)\n",
    "    out_path = os.path.join(CONTEXT_DIR, OUTPUT_FILE)\n",
    "    stats_path = os.path.join(CONTEXT_DIR, STATS_FILE)\n",
    "\n",
    "    if not os.path.exists(in_path):\n",
    "        raise FileNotFoundError(f\"Input file not found: {in_path}\")\n",
    "\n",
    "    kept_per_hop = defaultdict(list)\n",
    "    origin_counts = defaultdict(int)\n",
    "\n",
    "    print(f\"[INFO] Loading groups from {in_path}\")\n",
    "    with open(in_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "            hop = obj.get(\"hop\")\n",
    "            cands = obj.get(\"candidates\", [])\n",
    "            origin_counts[hop] += 1\n",
    "            if len(cands) == 3:\n",
    "                kept_per_hop[hop].append(obj)\n",
    "\n",
    "    filtered_counts = {h: len(lst) for h, lst in kept_per_hop.items()}\n",
    "\n",
    "    sampled = []\n",
    "    sampled_counts = {}\n",
    "    for hop, groups in kept_per_hop.items():\n",
    "        ratio = SAMPLE_RATIO.get(hop, 1.0)\n",
    "        if ratio >= 1.0:\n",
    "            chosen = groups\n",
    "        else:\n",
    "            n_total = len(groups)\n",
    "            want = ratio * n_total\n",
    "            if USE_FLOOR:\n",
    "                k = math.floor(want)\n",
    "            else:\n",
    "                k = round(want)\n",
    "            if n_total > 0 and k == 0:\n",
    "                k = 1  \n",
    "            if k < n_total:\n",
    "                chosen = random.sample(groups, k)\n",
    "            else:\n",
    "                chosen = groups\n",
    "        sampled.extend(chosen)\n",
    "        sampled_counts[hop] = len(chosen)\n",
    "\n",
    "    sampled.sort(key=lambda x: (x[\"target_real_id\"], x[\"hop\"], x[\"parent_label\"]))\n",
    "\n",
    "    with open(out_path, \"w\") as fout:\n",
    "        for obj in sampled:\n",
    "            fout.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    stats = {\n",
    "        \"input_file\": in_path,\n",
    "        \"output_file\": out_path,\n",
    "        \"random_seed\": RANDOM_SEED,\n",
    "        \"sample_ratio\": SAMPLE_RATIO,\n",
    "        \"origin_counts_per_hop\": dict(origin_counts),\n",
    "        \"filtered_counts_candidates_eq_3\": filtered_counts,\n",
    "        \"sampled_counts_per_hop\": sampled_counts,\n",
    "        \"total_sampled\": len(sampled)\n",
    "    }\n",
    "    with open(stats_path, \"w\") as fjson:\n",
    "        json.dump(stats, fjson, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"[INFO] Origin counts (all groups):\", dict(origin_counts))\n",
    "    print(\"[INFO] After length==3 filter:\", filtered_counts)\n",
    "    print(\"[INFO] Sampled counts:\", sampled_counts)\n",
    "    print(f\"[INFO] Total sampled groups written: {len(sampled)}\")\n",
    "    print(f\"[INFO] Stats written to: {stats_path}\")\n",
    "\n",
    "    print(\"[HINT] Sampled examples:\")\n",
    "    for obj in sampled[:5]:\n",
    "        print(json.dumps(obj, ensure_ascii=False))\n",
    "\n",
    "    print(\"[DONE] Sampling complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate basic information about all points involved, four for each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "build_prompts_from_groups.py\n",
    "----------------------------\n",
    "Given grouped path expansions (parent_label + 3 candidates),\n",
    "construct 4 English prompts per group:\n",
    "  * base prompt (parent path only)\n",
    "  * 3 extended prompts (parent path + each candidate)\n",
    "\n",
    "Input file  (JSONL):\n",
    "  path_groups_3hop_sampled.jsonl\n",
    "  Each line has: {\n",
    "      \"target_real_id\": \"...\",\n",
    "      \"hop\": 1|2|3,\n",
    "      \"parent_path\": [...],      # list of real ids (strings) (optional but we reconstruct from parent_label)\n",
    "      \"parent_label\": \"A|B|...\",\n",
    "      \"candidates\": [\"C\",\"D\",\"E\"],\n",
    "      ...\n",
    "  }\n",
    "\n",
    "External info sources:\n",
    "  COMPANY_INFO_PKL: dict[id] -> {\"basic_info\": str, \"label\": 0/1 or None}\n",
    "  INVESTOR_INFO_PKL: dict[id] -> str OR dict with key \"basic_info\"\n",
    "\n",
    "Conventions:\n",
    "  * First id of any path is always the target company (do NOT reveal its label).\n",
    "  * Paths alternate types company / investor / company / investor / ...\n",
    "  * Candidate list nodes are all the opposite type of the last node in parent path.\n",
    "  * For company nodes (except the target) we verbalize label:\n",
    "      1 -> \"previously succeeded\"\n",
    "      0 -> \"previously failed\"\n",
    "      missing -> \"outcome unknown\"\n",
    "\n",
    "Output JSONL:\n",
    "  prompts_3hop_groups.jsonl\n",
    "Each line:\n",
    "  {\n",
    "    \"target_id\": \"...\",\n",
    "    \"hop\": ...,\n",
    "    \"parent_label\": \"...\",\n",
    "    \"parent_path_ids\": [...],\n",
    "    \"base_prompt\": \"...\",\n",
    "    \"extended\": [\n",
    "        {\"candidate_id\": \"...\", \"extended_path_ids\": [...], \"prompt\": \"...\"},\n",
    "        ...\n",
    "    ]\n",
    "  }\n",
    "\n",
    "Adjust PATHS section below as needed.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "\n",
    "CONTEXT_DIR          = \"../graph_retrieval_data/context\"\n",
    "GROUPS_FILE          = \"path_groups_3hop_sampled.jsonl\"   \n",
    "OUTPUT_FILE          = \"prompts_3hop_groups.jsonl\"\n",
    "\n",
    "COMPANY_INFO_PKL     = \"../train_data_2022_basic.pkl\"\n",
    "INVESTOR_INFO_PKL    = \"../person_biography_dict.pkl\"\n",
    "\n",
    "PATH_SEPARATOR       = \"|\"     \n",
    "MAX_SHOW_EXAMPLES    = 3\n",
    "\n",
    "SYSTEM_PREAMBLE = (\n",
    "    \"You are a seasoned venture-capital investor. \"\n",
    "    \"A target company has just secured its Series-A financing. \"\n",
    "    \"Your task is to predict whether it will obtain a second round of financing, IPO, \"\n",
    "    \"or be acquired within the next 12 months.\\n\"\n",
    ")\n",
    "\n",
    "GUIDANCE_SUFFIX = (\n",
    "    \"\\nAbove is the target company's basic profile and a structured investment linkage context. \"\n",
    "    \"Based on ONLY this information, output your single-word judgment on whether the company will obtain a \"\n",
    "    \"second round, IPO, or be acquired within 12 months.\\n\"\n",
    "    \"Use exactly one of the following formats:\\n\"\n",
    "    \"  Prediction: True\\n\"\n",
    "    \"  Prediction: False\"\n",
    ")\n",
    "\n",
    "def is_investor(node_id: str) -> bool:\n",
    "    return node_id.endswith('P')\n",
    "\n",
    "def load_pickle(path: str):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def get_company_basic(company_info: Dict[str, Any], cid: str) -> str:\n",
    "    data = company_info.get(cid)\n",
    "    if data is None:\n",
    "        return f\"Basic information unavailable for company {cid}.\"\n",
    "    if isinstance(data, dict):\n",
    "        return data.get(\"basic_info\") or f\"Basic information unavailable for company {cid}.\"\n",
    "    if isinstance(data, str):\n",
    "        return data\n",
    "    return f\"Basic information unavailable for company {cid}.\"\n",
    "\n",
    "def get_company_label_text(company_info: Dict[str, Any], cid: str) -> str:\n",
    "    data = company_info.get(cid)\n",
    "    if not data or not isinstance(data, dict):\n",
    "        return \"outcome unknown\"\n",
    "    label = data.get(\"label\")\n",
    "    if label == 1:\n",
    "        return \"previously succeeded\"\n",
    "    if label == 0:\n",
    "        return \"previously failed\"\n",
    "    return \"outcome unknown\"\n",
    "\n",
    "def get_investor_info(investor_info: Dict[str, Any], iid: str) -> str:\n",
    "    data = investor_info.get(iid)\n",
    "    if data is None:\n",
    "        return f\"Background information unavailable for investor {iid}.\"\n",
    "    if isinstance(data, dict):\n",
    "        return data.get(\"basic_info\") or str(data)\n",
    "    if isinstance(data, str):\n",
    "        return data\n",
    "    return str(data)\n",
    "\n",
    "def describe_path_segment(path_ids: List[str],\n",
    "                          company_info: Dict[str, Any],\n",
    "                          investor_info: Dict[str, Any]) -> str:\n",
    "    if not path_ids:\n",
    "        return \"\"\n",
    "    target_id = path_ids[0]\n",
    "    target_basic = get_company_basic(company_info, target_id)\n",
    "\n",
    "    lines = []\n",
    "    lines.append(f\"Target company {target_id}: {target_basic}\")\n",
    "\n",
    "    for i in range(1, len(path_ids)):\n",
    "        prev_id = path_ids[i-1]\n",
    "        cur_id  = path_ids[i]\n",
    "        prev_is_inv = is_investor(prev_id)\n",
    "        cur_is_inv  = is_investor(cur_id)\n",
    "\n",
    "        if prev_is_inv and cur_is_inv:\n",
    "            lines.append(f\"NOTE: Two consecutive investors {prev_id}->{cur_id} (unexpected).\")\n",
    "        elif (not prev_is_inv) and (not cur_is_inv):\n",
    "            lines.append(f\"NOTE: Two consecutive companies {prev_id}->{cur_id} (unexpected).\")\n",
    "        else:\n",
    "            if cur_is_inv:\n",
    "                inv_info = get_investor_info(investor_info, cur_id)\n",
    "                if prev_id == target_id:\n",
    "                    lines.append(\n",
    "                        f\"Investor {cur_id} is one of the investors in target {target_id}. \"\n",
    "                        f\"Profile: {inv_info}\"\n",
    "                    )\n",
    "                else:\n",
    "                    lines.append(\n",
    "                        f\"Investor {cur_id} has also backed company {prev_id} in the chain. \"\n",
    "                        f\"Profile: {inv_info}\"\n",
    "                    )\n",
    "            else:\n",
    "                comp_basic = get_company_basic(company_info, cur_id)\n",
    "                comp_label_text = get_company_label_text(company_info, cur_id)\n",
    "                lines.append(\n",
    "                    f\"Company {cur_id} is another portfolio company linked via investor {prev_id}. \"\n",
    "                    f\"{comp_basic} Historical outcome: {comp_label_text}.\"\n",
    "                )\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def assemble_prompt(path_ids: List[str],\n",
    "                    company_info: Dict[str, Any],\n",
    "                    investor_info: Dict[str, Any]) -> str:\n",
    "    narrative = describe_path_segment(path_ids, company_info, investor_info)\n",
    "    body_intro = (\n",
    "        \"Below is investment-chain context you may consult to form your prediction.\\n\"\n",
    "    )\n",
    "    prompt = SYSTEM_PREAMBLE + body_intro + narrative + GUIDANCE_SUFFIX\n",
    "    return prompt\n",
    "\n",
    "def process():\n",
    "    company_info = load_pickle(COMPANY_INFO_PKL)\n",
    "    investor_info = load_pickle(INVESTOR_INFO_PKL)\n",
    "\n",
    "    groups_path = os.path.join(CONTEXT_DIR, GROUPS_FILE)\n",
    "    out_path    = os.path.join(CONTEXT_DIR, OUTPUT_FILE)\n",
    "\n",
    "    if not os.path.exists(groups_path):\n",
    "        raise FileNotFoundError(f\"Groups file not found: {groups_path}\")\n",
    "\n",
    "    total_groups = 0\n",
    "    written = 0\n",
    "\n",
    "    with open(groups_path, \"r\") as fin, open(out_path, \"w\") as fout:\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            group_obj = json.loads(line)\n",
    "            total_groups += 1\n",
    "\n",
    "            parent_label = group_obj[\"parent_label\"]\n",
    "            candidates   = group_obj.get(\"candidates\", [])\n",
    "            hop          = group_obj.get(\"hop\")\n",
    "            target_id    = group_obj.get(\"target_real_id\")\n",
    "\n",
    "            parent_path_ids = parent_label.split(PATH_SEPARATOR) if parent_label else []\n",
    "            if not parent_path_ids:\n",
    "                continue\n",
    "            if parent_path_ids[0] != target_id:\n",
    "                pass\n",
    "\n",
    "            base_prompt = assemble_prompt(parent_path_ids, company_info, investor_info)\n",
    "\n",
    "            extended_prompts = []\n",
    "            for cand_id in candidates:\n",
    "                extended_path = parent_path_ids + [cand_id]\n",
    "                ext_prompt = assemble_prompt(extended_path, company_info, investor_info)\n",
    "                extended_prompts.append({\n",
    "                    \"candidate_id\": cand_id,\n",
    "                    \"extended_path_ids\": extended_path,\n",
    "                    \"prompt\": ext_prompt\n",
    "                })\n",
    "\n",
    "            out_record = {\n",
    "                \"target_id\": target_id,\n",
    "                \"hop\": hop,\n",
    "                \"parent_label\": parent_label,\n",
    "                \"parent_path_ids\": parent_path_ids,\n",
    "                \"base_prompt\": base_prompt,\n",
    "                \"extended\": extended_prompts\n",
    "            }\n",
    "            fout.write(json.dumps(out_record, ensure_ascii=False) + \"\\n\")\n",
    "            written += 1\n",
    "\n",
    "    print(f\"[DONE] Processed groups: {total_groups}, wrote prompt records: {written}\")\n",
    "    print(f\"[OUTPUT] {out_path}\")\n",
    "\n",
    "    if MAX_SHOW_EXAMPLES > 0:\n",
    "        print(\"[HINT] Example prompt records:\")\n",
    "        with open(out_path, \"r\") as f:\n",
    "            for i, l in enumerate(f):\n",
    "                if i >= MAX_SHOW_EXAMPLES:\n",
    "                    break\n",
    "                print(l.strip()[:500] + (\"...\" if len(l.strip()) > 500 else \"\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Obtain offline reasoning structures for different information combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "from typing import Dict, List\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "CONTEXT_DIR = \"../graph_retrieval_data/context\"\n",
    "PROMPTS_FILE = \"prompts_3hop_groups.jsonl\"\n",
    "\n",
    "MODEL_PATH = \"../LLama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "DEVICE_MAP = \"auto\"\n",
    "DTYPE = torch.bfloat16   \n",
    "MAX_SHOW_TOKENS = 2      \n",
    "ADD_DEBUG_GENERATE = False \n",
    "\n",
    "print(\"[INFO] Loading model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    use_fast=False,\n",
    "    local_files_only=True\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=DTYPE,\n",
    "    device_map=DEVICE_MAP,\n",
    "    local_files_only=True\n",
    ")\n",
    "model.eval()\n",
    "print(\"[INFO] Model loaded.\")\n",
    "\n",
    "TRUE_TOKENS = tokenizer.encode(\"True\", add_special_tokens=False)\n",
    "FALSE_TOKENS = tokenizer.encode(\"False\", add_special_tokens=False)\n",
    "print(f\"[DEBUG] TRUE_TOKENS={TRUE_TOKENS}, FALSE_TOKENS={FALSE_TOKENS}\")\n",
    "\n",
    "def ensure_prediction_suffix(prompt: str) -> str:\n",
    "    suffix = \"Prediction: \"\n",
    "    if prompt.rstrip().endswith(\"Prediction:\"):\n",
    "        return prompt.rstrip() + \" \"\n",
    "    if prompt.endswith(suffix):\n",
    "        return prompt\n",
    "    if not prompt.endswith(\"\\n\"):\n",
    "        prompt += \"\\n\"\n",
    "    return prompt + suffix\n",
    "\n",
    "@torch.no_grad()\n",
    "def two_token_conditional_prob(prompt: str) -> Dict[str, float]:\n",
    "    prompt = ensure_prediction_suffix(prompt)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "    out = model(**inputs)\n",
    "    logits_last = out.logits[:, -1, :]  # (1, vocab)\n",
    "    probs_first = F.softmax(logits_last, dim=-1)\n",
    "\n",
    "    if len(TRUE_TOKENS) == 1 and len(FALSE_TOKENS) == 1:\n",
    "        p_true_raw = probs_first[0, TRUE_TOKENS[0]]\n",
    "        p_false_raw = probs_first[0, FALSE_TOKENS[0]]\n",
    "    else:\n",
    "        def seq_prob(prefix_ids: torch.Tensor, seq: List[int]) -> torch.Tensor:\n",
    "            cur = prefix_ids.clone()\n",
    "            logp = 0.0\n",
    "            for tid in seq:\n",
    "                out_step = model(input_ids=cur)\n",
    "                logits_step = out_step.logits[:, -1, :]\n",
    "                probs_step = F.softmax(logits_step, dim=-1)\n",
    "                p_tok = probs_step[0, tid]\n",
    "                logp += torch.log(p_tok + 1e-12)\n",
    "                next_tok = torch.tensor([[tid]], device=cur.device)\n",
    "                cur = torch.cat([cur, next_tok], dim=1)\n",
    "            return torch.exp(logp)\n",
    "\n",
    "        p_true_raw = seq_prob(input_ids, TRUE_TOKENS)\n",
    "        p_false_raw = seq_prob(input_ids, FALSE_TOKENS)\n",
    "\n",
    "    denom = p_true_raw + p_false_raw + 1e-12\n",
    "    p_true = (p_true_raw / denom).item()\n",
    "    p_false = (p_false_raw / denom).item()\n",
    "\n",
    "    log_odds = math.log((p_true + 1e-12) / (p_false + 1e-12))\n",
    "    margin = p_true - p_false\n",
    "\n",
    "    return {\n",
    "        \"prompt_final\": prompt,\n",
    "        \"p_true\": p_true,\n",
    "        \"p_false\": p_false,\n",
    "        \"log_odds\": log_odds,\n",
    "        \"margin\": margin,\n",
    "        \"pred_label\": \"True\" if p_true >= 0.5 else \"False\"\n",
    "    }\n",
    "\n",
    "@torch.no_grad()\n",
    "def debug_generate(prompt: str):\n",
    "    prompt = ensure_prediction_suffix(prompt)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    gen = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=MAX_SHOW_TOKENS,\n",
    "        temperature=0.0,\n",
    "        do_sample=False\n",
    "    )\n",
    "    full = gen.sequences[0]\n",
    "    new_tokens = full[inputs[\"input_ids\"].shape[1]:]\n",
    "    txt = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "    return txt.strip()\n",
    "\n",
    "def load_first_group(path: str):\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "            return obj\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    groups_path = os.path.join(CONTEXT_DIR, PROMPTS_FILE)\n",
    "    group = load_first_group(groups_path)\n",
    "    if group is None:\n",
    "        print(\"[ERROR] No group found.\")\n",
    "        return\n",
    "\n",
    "    print(\"[INFO] Loaded one group:\")\n",
    "    print(json.dumps({\n",
    "        \"target_id\": group[\"target_id\"],\n",
    "        \"hop\": group[\"hop\"],\n",
    "        \"parent_label\": group[\"parent_label\"],\n",
    "        \"num_candidates\": len(group.get(\"extended\", []))\n",
    "    }, ensure_ascii=False, indent=2))\n",
    "\n",
    "    base_prompt = group[\"base_prompt\"]\n",
    "    base_res = two_token_conditional_prob(base_prompt)\n",
    "\n",
    "    result_record = {\n",
    "        \"target_id\": group[\"target_id\"],\n",
    "        \"hop\": group[\"hop\"],\n",
    "        \"parent_label\": group[\"parent_label\"],\n",
    "        \"base\": {\n",
    "            \"p_true\": base_res[\"p_true\"],\n",
    "            \"p_false\": base_res[\"p_false\"],\n",
    "            \"log_odds\": base_res[\"log_odds\"],\n",
    "            \"margin\": base_res[\"margin\"],\n",
    "            \"pred_label\": base_res[\"pred_label\"]\n",
    "        },\n",
    "        \"extended\": []\n",
    "    }\n",
    "\n",
    "    if ADD_DEBUG_GENERATE:\n",
    "        dbg = debug_generate(base_prompt)\n",
    "        print(f\"[DEBUG] Base generated: {dbg}\")\n",
    "\n",
    "    for ext in group[\"extended\"]:\n",
    "        cand_id = ext[\"candidate_id\"]\n",
    "        prompt_ext = ext[\"prompt\"]\n",
    "        ext_res = two_token_conditional_prob(prompt_ext)\n",
    "        if ADD_DEBUG_GENERATE:\n",
    "            dbg2 = debug_generate(prompt_ext)\n",
    "            print(f\"[DEBUG] Ext {cand_id} generated: {dbg2}\")\n",
    "\n",
    "        result_record[\"extended\"].append({\n",
    "            \"candidate_id\": cand_id,\n",
    "            \"p_true\": ext_res[\"p_true\"],\n",
    "            \"p_false\": ext_res[\"p_false\"],\n",
    "            \"log_odds\": ext_res[\"log_odds\"],\n",
    "            \"margin\": ext_res[\"margin\"],\n",
    "            \"pred_label\": ext_res[\"pred_label\"]\n",
    "        })\n",
    "\n",
    "    print(\"\\n[RESULT]\")\n",
    "    print(json.dumps(result_record, ensure_ascii=False, indent=2))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The following is the batch processing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "batch_infer_prompts.py\n",
    "----------------------\n",
    "Batch inference for all prompt groups:\n",
    "  - Reads prompts_3hop_groups.jsonl\n",
    "  - Computes conditional probabilities p(True), p(False) for base + each extended prompt\n",
    "  - (Optional) Computes Î” gain for each extended w.r.t. base if target label is available\n",
    "  - Writes predictions_3hop_groups.jsonl\n",
    "\n",
    "  CE(y,p) = - [ y log p + (1-y) log(1-p) ]\n",
    "  Î” = (CE_base - CE_ext) + Î» * ( |p_ext - 0.5| - |p_base - 0.5| )\n",
    "\n",
    "Requires:\n",
    "  - Llama causal LM\n",
    "  - (Optional) company_info.pkl: {company_id: {\"label\": 0/1, ...}}\n",
    "\n",
    "Resume:\n",
    "  If output exists and RESUME=True, already processed keys won't be recomputed.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import pickle\n",
    "import hashlib\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "\n",
    "CONTEXT_DIR   = \"../graph_retrieval_data/context\"\n",
    "INPUT_FILE    = \"prompts_3hop_groups.jsonl\"\n",
    "OUTPUT_FILE   = \"predictions_3hop_groups.jsonl\"\n",
    "\n",
    "COMPANY_INFO_PKL = \"../train_data_2022_basic.pkl\"\n",
    "USE_COMPANY_LABEL = True          \n",
    "LAMBDA_CONF = 0.2                 \n",
    "\n",
    "MODEL_PATH   = \"/data/VC_LLM_Agent/LLama/Meta-Llama-3.1-8B-Instruct\"\n",
    "DTYPE        = torch.bfloat16    \n",
    "DEVICE_MAP   = \"auto\"\n",
    "\n",
    "BATCH_SIZE   = 1                 \n",
    "RESUME       = True              \n",
    "PRED_SUFFIX  = \"Prediction: \"    \n",
    "CACHE_MAX    = 200000             \n",
    "\n",
    "COMPUTE_DELTA = True             \n",
    "EPS = 1e-12\n",
    "\n",
    "SHOW_PROGRESS = True\n",
    "PRINT_EVERY   = 2000             \n",
    "\n",
    "MAX_CTX     = 2048      \n",
    "KEEP_HEAD   = 1024     \n",
    "\n",
    "def ensure_prediction_suffix(prompt: str) -> str:\n",
    "    if prompt.endswith(PRED_SUFFIX):\n",
    "        return prompt\n",
    "    if prompt.rstrip().endswith(PRED_SUFFIX.strip()):\n",
    "        return prompt.rstrip() + \" \"\n",
    "    if not prompt.endswith(\"\\n\"):\n",
    "        prompt += \"\\n\"\n",
    "    return prompt + PRED_SUFFIX\n",
    "\n",
    "def load_company_info(path: str) -> Dict[str, Any]:\n",
    "    if not USE_COMPANY_LABEL:\n",
    "        return {}\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"[WARN] company info file not found: {path}; Î” will be skipped.\")\n",
    "        return {}\n",
    "    with open(path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    return data if isinstance(data, dict) else {}\n",
    "    \n",
    "\n",
    "def get_label(company_info: Dict[str, Any], cid: str):\n",
    "    rec = company_info.get(cid)\n",
    "    if not rec or not isinstance(rec, dict):\n",
    "        return None\n",
    "    return rec.get(\"label\")\n",
    "\n",
    "def ce_loss(y: int, p: float, eps=1e-12):\n",
    "    p = min(max(p, eps), 1 - eps)\n",
    "    return -(y * math.log(p) + (1 - y) * math.log(1 - p))\n",
    "\n",
    "def compute_delta(y: int, p_base: float, p_ext: float, lambda_conf: float) -> float:\n",
    "    ce_b = ce_loss(y, p_base)\n",
    "    ce_e = ce_loss(y, p_ext)\n",
    "    return (ce_b - ce_e) + lambda_conf * (abs(p_ext - 0.5) - abs(p_base - 0.5))\n",
    "\n",
    "def model_setup():\n",
    "    print(\"[INFO] Loading model...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        use_fast=False,\n",
    "        local_files_only=True\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        torch_dtype=DTYPE,\n",
    "        device_map=DEVICE_MAP,\n",
    "        local_files_only=True\n",
    "    )\n",
    "    model.eval()\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = tokenizer.eos_token_id\n",
    "        print(f\"[INFO] Set pad_token to eos_token (id={tokenizer.pad_token_id}).\")\n",
    "\n",
    "\n",
    "    true_tokens  = tokenizer.encode(\"True\", add_special_tokens=False)\n",
    "    false_tokens = tokenizer.encode(\"False\", add_special_tokens=False)\n",
    "    print(f\"[INFO] TRUE tokens={true_tokens}, FALSE tokens={false_tokens}\")\n",
    "    return tokenizer, model, true_tokens, false_tokens\n",
    "\n",
    "\n",
    "def maybe_truncate(prompt: str, tok: AutoTokenizer) -> str:\n",
    "    ids = tok.encode(prompt, add_special_tokens=False)\n",
    "    if len(ids) <= MAX_CTX:\n",
    "        return prompt\n",
    "    head = ids[:KEEP_HEAD]\n",
    "    tail = ids[-(MAX_CTX - KEEP_HEAD):]\n",
    "    new_ids = head + tail\n",
    "    return tok.decode(new_ids, skip_special_tokens=True)\n",
    "\n",
    "def seq_raw_prob(model, tokenizer, prefix_ids: torch.Tensor, target_ids: List[int]) -> torch.Tensor:\n",
    "    cur = prefix_ids\n",
    "    logp = 0.0\n",
    "    for tid in target_ids:\n",
    "        out = model(input_ids=cur, use_cache=False)\n",
    "        logits = out.logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        p_tok = probs[0, tid]\n",
    "        logp = logp + torch.log(p_tok + 1e-12)\n",
    "        next_tok = torch.tensor([[tid]], device=cur.device)\n",
    "        cur = torch.cat([cur, next_tok], dim=1)\n",
    "    return torch.exp(logp)\n",
    "\n",
    "def batch_boolean_probs(model,\n",
    "                        tokenizer,\n",
    "                        prompts: List[str],\n",
    "                        true_tokens: List[int],\n",
    "                        false_tokens: List[int]) -> List[Dict[str, float]]:\n",
    "    multi = (len(true_tokens) > 1) or (len(false_tokens) > 1)\n",
    "    results = []\n",
    "    if multi:\n",
    "        for pr in prompts:\n",
    "            pr_final = ensure_prediction_suffix(pr)\n",
    "            enc = tokenizer(pr_final, return_tensors=\"pt\").to(model.device)\n",
    "            p_true_raw = seq_raw_prob(model, tokenizer, enc[\"input_ids\"], true_tokens)\n",
    "            p_false_raw = seq_raw_prob(model, tokenizer, enc[\"input_ids\"], false_tokens)\n",
    "            denom = p_true_raw + p_false_raw + 1e-12\n",
    "            p_true = (p_true_raw / denom).item()\n",
    "            p_false = (p_false_raw / denom).item()\n",
    "            results.append({\"p_true\": p_true, \"p_false\": p_false})\n",
    "        return results\n",
    "\n",
    "    fixed_prompts = [\n",
    "    maybe_truncate(ensure_prediction_suffix(p), tokenizer)   \n",
    "    for p in prompts\n",
    "    ]\n",
    "    enc = tokenizer(fixed_prompts, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "    with torch.inference_mode():\n",
    "        out = model(**enc, use_cache=False)\n",
    "    logits = out.logits  # (B, L, V)\n",
    "    attn = enc[\"attention_mask\"]\n",
    "    last_idx = attn.sum(dim=1) - 1  # (B,)\n",
    "\n",
    "    true_id = true_tokens[0]\n",
    "    false_id = false_tokens[0]\n",
    "\n",
    "    for i in range(len(fixed_prompts)):\n",
    "        z = logits[i, last_idx[i], :]\n",
    "        probs = F.softmax(z, dim=-1)\n",
    "        p_true_raw = probs[true_id]\n",
    "        p_false_raw = probs[false_id]\n",
    "        denom = p_true_raw + p_false_raw + 1e-12\n",
    "        p_true = (p_true_raw / denom).item()\n",
    "        p_false = (p_false_raw / denom).item()\n",
    "        first_tok_id = int(torch.argmax(z))\n",
    "        first_tok    = tokenizer.decode([first_tok_id], skip_special_tokens=True).strip()\n",
    "        results.append({\"p_true\": p_true, \n",
    "                        \"p_false\": p_false,\n",
    "                        \"first_token\": first_tok})\n",
    "    return results\n",
    "\n",
    "def prompt_hash(s: str) -> str:\n",
    "    return hashlib.md5(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def load_existing_predictions(path: str) -> Dict[str, Any]:\n",
    "    if not os.path.exists(path):\n",
    "        return {}\n",
    "    existing = {}\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "            key = f\"{obj['target_id']}|{obj['hop']}|{obj['parent_label']}\"\n",
    "            existing[key] = obj\n",
    "    print(f\"[INFO] Resume enabled: loaded {len(existing)} existing group records.\")\n",
    "    return existing\n",
    "\n",
    "def main():\n",
    "    tokenizer, model, true_tokens, false_tokens = model_setup()\n",
    "    company_info = load_company_info(COMPANY_INFO_PKL) if USE_COMPANY_LABEL else {}\n",
    "\n",
    "    input_path = os.path.join(CONTEXT_DIR, INPUT_FILE)\n",
    "    output_path = os.path.join(CONTEXT_DIR, OUTPUT_FILE)\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(input_path)\n",
    "\n",
    "    existing = load_existing_predictions(output_path) if RESUME else {}\n",
    "\n",
    "    fout = open(output_path, \"a\" if RESUME else \"w\")\n",
    "\n",
    "    prob_cache: OrderedDict[str, Dict[str, float]] = OrderedDict()\n",
    "\n",
    "    def cache_get_or_none(ph):\n",
    "        return prob_cache.get(ph)\n",
    "\n",
    "    def cache_set(ph, val):\n",
    "        prob_cache[ph] = val\n",
    "        if len(prob_cache) > CACHE_MAX:\n",
    "            prob_cache.popitem(last=False)\n",
    "\n",
    "    total_groups = 0\n",
    "    processed_groups = 0\n",
    "    missing_label = 0\n",
    "\n",
    "    batch_prompts: List[str] = []\n",
    "    batch_meta: List[Tuple[str, str, str, str]] = []\n",
    "\n",
    "    def flush_batch():\n",
    "        nonlocal batch_prompts, batch_meta\n",
    "        if not batch_prompts:\n",
    "            return\n",
    "        probs_list = batch_boolean_probs(\n",
    "            model, tokenizer,\n",
    "            batch_prompts, true_tokens, false_tokens\n",
    "        )\n",
    "        for meta, probs in zip(batch_meta, probs_list):\n",
    "            ph, kind, gkey, cid = meta\n",
    "            cache_set(ph, probs)\n",
    "        batch_prompts.clear()\n",
    "        batch_meta.clear()\n",
    "\n",
    "    with open(input_path, \"r\") as fin:\n",
    "        for line in tqdm(fin, desc=\"Groups\", disable=not SHOW_PROGRESS):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            g = json.loads(line)\n",
    "            total_groups += 1\n",
    "\n",
    "            gkey = f\"{g['target_id']}|{g['hop']}|{g['parent_label']}\"\n",
    "            if RESUME and gkey in existing:\n",
    "                continue  \n",
    "\n",
    "            base_prompt = g[\"base_prompt\"]\n",
    "            exts = g.get(\"extended\", [])\n",
    "\n",
    "            base_prompt_final = maybe_truncate(\n",
    "                ensure_prediction_suffix(base_prompt), tokenizer)\n",
    "            ph_base = prompt_hash(base_prompt_final)\n",
    "            p_base_entry = cache_get_or_none(ph_base)\n",
    "            if p_base_entry is None:\n",
    "                batch_prompts.append(base_prompt_final)\n",
    "                batch_meta.append((ph_base, \"base\", gkey, \"BASE\"))\n",
    "\n",
    "            for ext in exts:\n",
    "                ep = maybe_truncate(\n",
    "                    ensure_prediction_suffix(ext[\"prompt\"]), tokenizer)\n",
    "                ph_ext = prompt_hash(ep)\n",
    "                p_ext_entry = cache_get_or_none(ph_ext)\n",
    "                if p_ext_entry is None:\n",
    "                    batch_prompts.append(ep)\n",
    "                    batch_meta.append((ph_ext, \"ext\", gkey, ext[\"candidate_id\"]))\n",
    "\n",
    "            if len(batch_prompts) >= BATCH_SIZE:\n",
    "                flush_batch()\n",
    "\n",
    "            existing[gkey] = {\n",
    "                \"_raw_group\": g,\n",
    "                \"_need_finalize\": True\n",
    "            }\n",
    "\n",
    "    flush_batch()\n",
    "\n",
    "    for gkey, record in tqdm(existing.items(), desc=\"Finalize\", disable=not SHOW_PROGRESS):\n",
    "        if not isinstance(record, dict):\n",
    "            continue\n",
    "        if not record.get(\"_need_finalize\"):\n",
    "            continue\n",
    "        g = record[\"_raw_group\"]\n",
    "        target_id = g[\"target_id\"]\n",
    "        hop = g[\"hop\"]\n",
    "        parent_label = g[\"parent_label\"]\n",
    "\n",
    "        base_prompt = maybe_truncate(\n",
    "            ensure_prediction_suffix(g[\"base_prompt\"]), tokenizer)\n",
    "        ph_base = prompt_hash(base_prompt)\n",
    "        base_probs = cache_get_or_none(ph_base)\n",
    "        if base_probs is None:\n",
    "            raise RuntimeError(\"Cache miss for base prompt.\")\n",
    "\n",
    "        p_base_true = base_probs[\"p_true\"]\n",
    "        p_base_false = base_probs[\"p_false\"]\n",
    "        pred_base = \"True\" if p_base_true >= 0.5 else \"False\"\n",
    "\n",
    "        label = None\n",
    "        if COMPUTE_DELTA and USE_COMPANY_LABEL:\n",
    "            label = get_label(company_info, target_id)\n",
    "            if label not in (0, 1):\n",
    "                label = None\n",
    "                missing_label += 1\n",
    "\n",
    "        extended_out = []\n",
    "        for ext in g.get(\"extended\", []):\n",
    "            ep = maybe_truncate(\n",
    "                ensure_prediction_suffix(ext[\"prompt\"]), tokenizer)\n",
    "            ph_ext = prompt_hash(ep)\n",
    "            ext_probs = cache_get_or_none(ph_ext)\n",
    "            if ext_probs is None:\n",
    "                raise RuntimeError(\"Cache miss for extended prompt.\")\n",
    "            p_ext_true = ext_probs[\"p_true\"]\n",
    "            p_ext_false = ext_probs[\"p_false\"]\n",
    "            pred_ext = \"True\" if p_ext_true >= 0.5 else \"False\"\n",
    "\n",
    "            delta_val = None\n",
    "            if COMPUTE_DELTA and label is not None:\n",
    "                delta_val = compute_delta(label, p_base_true, p_ext_true, LAMBDA_CONF)\n",
    "\n",
    "            extended_out.append({\n",
    "                \"candidate_id\": ext[\"candidate_id\"],\n",
    "                \"p_true\": p_ext_true,\n",
    "                \"p_false\": p_ext_false,\n",
    "                \"pred_label\": pred_ext,\n",
    "                \"delta\": delta_val\n",
    "            })\n",
    "\n",
    "        out_obj = {\n",
    "            \"target_id\": target_id,\n",
    "            \"hop\": hop,\n",
    "            \"parent_label\": parent_label,\n",
    "            \"p_true_base\": p_base_true,\n",
    "            \"p_false_base\": p_base_false,\n",
    "            \"pred_label_base\": pred_base,\n",
    "            \"label\": label,  \n",
    "            \"lambda_conf\": LAMBDA_CONF if COMPUTE_DELTA else None,\n",
    "            \"extended\": extended_out\n",
    "        }\n",
    "        fout.write(json.dumps(out_obj, ensure_ascii=False) + \"\\n\")\n",
    "        processed_groups += 1\n",
    "        if processed_groups % PRINT_EVERY == 0:\n",
    "            print(f\"[INFO] Written {processed_groups} groups...\")\n",
    "\n",
    "    fout.close()\n",
    "\n",
    "    print(f\"\\n[DONE] Total groups in file      : {total_groups}\")\n",
    "    print(f\"[DONE] Newly processed groups    : {processed_groups}\")\n",
    "    if COMPUTE_DELTA and USE_COMPANY_LABEL:\n",
    "        print(f\"[INFO] Groups missing label (Î” skipped): {missing_label}\")\n",
    "    print(f\"[OUTPUT] {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Obtain the text embedding of the corresponding information segment to facilitate the subsequent training of the selector based on semantic judgment of information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os, json, pickle, hashlib\n",
    "from collections import OrderedDict\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "PROMPT_JSONL = \"../graph_retrieval_data/context/prompts_3hop_groups.jsonl\"\n",
    "SAVE_DIR     = \"../graph_retrieval_data/context\"\n",
    "OUT_PKL      = os.path.join(SAVE_DIR, \"prompt_emb.pkl\")\n",
    "\n",
    "BATCH_SIZE   = 1         \n",
    "DTYPE        = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "DEVICE       = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "MAX_TOKENS   = 4096\n",
    "HALF_TOKENS  = MAX_TOKENS // 2\n",
    "\n",
    "print(\"[INFO] loading jinaai/jina-embeddings-v2-base-en â€¦\")\n",
    "tok = AutoTokenizer.from_pretrained(\"jinaai/jina-embeddings-v2-base-en\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"jinaai/jina-embeddings-v2-base-en\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ").to(DEVICE, dtype=DTYPE)\n",
    "model.eval()\n",
    "print(\"[INFO] model loaded.\")\n",
    "\n",
    "if os.path.exists(OUT_PKL):\n",
    "    with open(OUT_PKL, \"rb\") as f:\n",
    "        emb_cache: Dict[str, np.ndarray] = pickle.load(f)\n",
    "    print(f\"[INFO] cache loaded: {len(emb_cache)} embeddings\")\n",
    "else:\n",
    "    emb_cache = OrderedDict()\n",
    "\n",
    "def md5(s: str) -> str:\n",
    "    return hashlib.md5(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def trim_text_to_max_tokens(text: str) -> str:\n",
    "    tokens = tok.encode(text, add_special_tokens=False)\n",
    "    if len(tokens) > MAX_TOKENS:\n",
    "        tokens = tokens[:HALF_TOKENS] + tokens[-HALF_TOKENS:]\n",
    "    return tok.decode(tokens, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "def encode_batch(texts: List[str]) -> List[np.ndarray]:\n",
    "    texts = [trim_text_to_max_tokens(t) for t in texts]\n",
    "    with torch.no_grad():\n",
    "        enc = tok(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_TOKENS,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(DEVICE)\n",
    "        out = model(**enc)\n",
    "        cls = out.last_hidden_state[:, 0, :]\n",
    "        cls = torch.nn.functional.normalize(cls, p=2, dim=1)\n",
    "        return [v.cpu().float().numpy() for v in cls]\n",
    "\n",
    "batch_txt, batch_hash = [], []\n",
    "new_cnt = 0\n",
    "\n",
    "def flush():\n",
    "    global batch_txt, batch_hash, new_cnt\n",
    "    if not batch_txt: return\n",
    "    vecs = encode_batch(batch_txt)\n",
    "    for h, v in zip(batch_hash, vecs):\n",
    "        emb_cache[h] = v\n",
    "    new_cnt += len(batch_txt)\n",
    "    batch_txt, batch_hash = [], []\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "with open(PROMPT_JSONL) as f:\n",
    "    for line in tqdm(f, desc=\"Scanning prompts\"):\n",
    "        obj = json.loads(line)\n",
    "        h = md5(obj[\"base_prompt\"])\n",
    "        if h not in emb_cache:\n",
    "            batch_txt.append(obj[\"base_prompt\"]); batch_hash.append(h)\n",
    "        for ext in obj[\"extended\"]:\n",
    "            h2 = md5(ext[\"prompt\"])\n",
    "            if h2 not in emb_cache:\n",
    "                batch_txt.append(ext[\"prompt\"]); batch_hash.append(h2)\n",
    "        if len(batch_txt) >= BATCH_SIZE:\n",
    "            flush()\n",
    "\n",
    "flush() \n",
    "\n",
    "print(f\"[INFO] new embeddings added: {new_cnt}; total: {len(emb_cache)}\")\n",
    "\n",
    "tmp_out = OUT_PKL + \".tmp\"\n",
    "with open(tmp_out, \"wb\") as f:\n",
    "    pickle.dump(emb_cache, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "os.replace(tmp_out, OUT_PKL)\n",
    "print(f\"[DONE] embeddings saved to {OUT_PKL}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VC_LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
